{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c915a1c9",
   "metadata": {},
   "source": [
    "# Retrieval‑Augmented Generation (RAG) Workshop & Notebook Guide  \n",
    "*A step‑by‑step companion for developers new to GenAI*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8811f5c",
   "metadata": {},
   "source": [
    "## 0  |  Purpose of this Notebook\n",
    "\n",
    "The goal of the workshop is to **contrast “plain prompting” with Retrieval‑Augmented Generation (RAG)** and let you experience, hands‑on, how adding a retrieval step and function‑calling tools makes GPT‑4o‑mini:\n",
    "\n",
    "* **more accurate** (grounded answers, fewer hallucinations)  \n",
    "* **faster & cheaper** on larger corpora  \n",
    "* **deterministic** when you need it (e.g. function calls)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0179bf8",
   "metadata": {},
   "source": [
    "## 1  |  Getting Ready\n",
    "\n",
    "| What | Why |\n",
    "|------|-----|\n",
    "| **Python ≥ 3.10** | required by LangChain 0.2+ |\n",
    "| **OpenAI account & `OPENAI_API_KEY`** | to access `gpt‑4o‑mini` |\n",
    "| **Packages** | `pip install -r requirements.txt` |\n",
    "| **Workshop data** | 50 short markdown files (≈ 90 kB total) provided in `/data/docs/` |\n",
    "\n",
    "Clone the repo or pull the zipped folder, then open **`RAG‑workshop.ipynb`** in Jupyter or VS Code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ce25e8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run tot install python packages.\n",
    "%pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "740bdbe9",
   "metadata": {},
   "source": [
    "### 1.1. LLM call, OpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "9960f694",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sure! Here’s \"Hello world\" in five different languages:\n",
      "\n",
      "1. Spanish: ¡Hola mundo!\n",
      "2. French: Bonjour le monde !\n",
      "3. German: Hallo Welt!\n",
      "4. Italian: Ciao mondo!\n",
      "5. Japanese: こんにちは世界 (Konnichiwa sekai)! \n",
      "\n",
      "Let me know if you need anything else!\n"
     ]
    }
   ],
   "source": [
    "#Prompting API example.\n",
    "import importlib,utils\n",
    "importlib.reload(utils)\n",
    "from utils import gpt_4o_mini\n",
    "\n",
    "query = \"Say 'Hello world' in five different languages.\"\n",
    "print(gpt_4o_mini(query).content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a03d1926",
   "metadata": {},
   "source": [
    "### 1.2. Baseline data, Pensioenswet\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "ab65005e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Firm:data/docs\\Pensioenwet.txt \n",
      "Text:\n",
      "\n",
      "Pensioenwet\n",
      "\n",
      "Artikel 17. Gelijke premie\n",
      "\n",
      "1.\tDe door of voor een deelnemer verschuldigde premie voor pensioen op opbouwbasis bedraagt voor alle deelnemers een gelijk percentage van het loon dat voor de pensioenberekening in aanmerking wordt genomen.\n",
      "\n",
      "2.\tDe door of voor een deelnemer verschuldigde premie voor pensioen op risicobasis bedraagt bij een pensioenregeling uitgevoerd door een verplichtgesteld bedrijfstakpensioenfonds een gelijk percentage van het loon dat voor de pensioenberekening in aanmerking wordt genomen.\n",
      "\n",
      "3.\tVoor verschillende vormen van pensioen en voor verschillende pensioenregelingen kunnen verschillende premies worden vastgesteld. Voor verschillende pensioenregelingen die worden uitgevoerd door hetzelfde verplichtgestelde bedrijfstakpensioenfonds kunnen geen verschillende premies worden vastgesteld indien die pensioenregelingen dezelfde of nagenoeg dezelfde inhoud hebben.\n",
      "\n",
      "4.\tHet eerste en tweede lid zijn niet van toepassing op de premie voor vrijwillige pensioenregelingen.\n",
      "\n",
      "5.\tDe tweede zin van het derde lid is niet van toepassing indien bij een verplichtgesteld bedrijfstakpensioenfonds sprake is van een regeling die voor alle deelnemers geldt en de actuariële waarde van de uit die regeling voortvloeiende verplichtingen ten minste twee derde van de actuariële waarde van de uit het totaal van de pensioenregelingen van het bedrijfstakpensioenfonds anders dan regelingen voor nabestaanden- of arbeidsongeschiktheidspensioen, voortvloeiende verplichtingen met uitzondering van de uit de vrijwillige pensioenvoorzieningen voortvloeiende verplichtingen, betreft.\n",
      "\n",
      "Artikel 220e. Overgangsrecht progressieve premie\n",
      "\n",
      "1.\tIn afwijking van artikel 17 mag de door of voor een deelnemer verschuldigde premie tot het moment van beëindiging van de deelneming een met de leeftijd oplopend percentage van het loon dat voor de pensioenberekening in aanmerking wordt genomen bedragen, mits:\n",
      "\ta.\top de dag voorafgaand aan het tijdstip van inwerkingtreding van de Wet toekomst pensioenen sprake was van een premieovereenkomst met een met de leeftijd oplopend premiepercentage of een uitkeringsovereenkomst als bedoeld in artikel 1, zoals dat artikel luidde op de dag voorafgaand aan het tijdstip van inwerkingtreding van de Wet toekomst pensioenen, met een met de leeftijd oplopend premiepercentage ondergebracht bij een verzekeraar;\n",
      "\tb.\tde deelneming van de deelnemer reeds was aangevangen waarbij de deelnemer pensioenaanspraken opbouwt, op de dag voordat voor nieuwe deelnemers een pensioenovereenkomst geldt waarbij de premie conform artikel 17 voor alle deelnemers een gelijk percentage van het loon dat voor de pensioenberekening in aanmerking wordt genomen bedraagt, doch uiterlijk op 31 december 2026; en\n",
      "\tc.\tde pensioenovereenkomst niet het karakter heeft van een solidaire premieovereenkomst.\n",
      " \n"
     ]
    }
   ],
   "source": [
    "import glob\n",
    "\n",
    "# Index of file to be printed\n",
    "file_index = 1\n",
    "\n",
    "for filepath in glob.glob(\"data/docs/*.txt\")[file_index:file_index+1]:\n",
    "    with open(filepath, \"r\", encoding=\"utf-8\") as f:\n",
    "        print(f\"Firm:{filepath} \\nText:{f.read()} \")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "4b58a52f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AOW.txt: 300 tokens\n",
      "Pensioenwet.txt: 840 tokens\n",
      "Uitvoeringsbesluit loonbelasting 1965.txt: 837 tokens\n",
      "Wet op loonbelasting 1964.txt: 1153 tokens\n",
      "Wet Toekomst Pensioenen.txt: 3344 tokens\n",
      "Wet verplichte beroepspensioenregeling.txt: 519 tokens\n",
      "\n",
      "Total tokens across all files: 6993\n"
     ]
    }
   ],
   "source": [
    "# Token count\n",
    "import importlib, utils\n",
    "importlib.reload(utils)\n",
    "from utils import count_tokens_in_docs\n",
    "\n",
    "print(count_tokens_in_docs())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38700875",
   "metadata": {},
   "source": [
    "## 2  |  Baselines — No RAG\n",
    "\n",
    "### 2.1  Naïve “Stuff‑Everything” Prompt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f77b5f50",
   "metadata": {},
   "source": [
    "Chunk all text together, question answered correctly at random."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "4c6a340a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Response: The minimal applicable franchise-value, given a premium percentage of 27%, would be €18,475, as specified in Article 18a of the Wet op de loonbelasting 1964.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import importlib, utils\n",
    "importlib.reload(utils)\n",
    "from utils import gpt_4o_mini\n",
    "import glob\n",
    "# TODO: Increase size of text chunks to demonstrate this.\n",
    "\n",
    "# Chunks all text files together in a single string.\n",
    "text = \"\"\n",
    "for filepath in glob.glob(\"data/docs/*.txt\"):\n",
    "    with open(filepath, \"r\", encoding=\"utf-8\") as f:\n",
    "        text += f.read()\n",
    "\n",
    "SYSTEM = f\"\"\"The text between triple backtics are multiple Dutch law texts, I will ask retrieval-based questions about them: ```{text}```\"\"\"\n",
    "query = \"In case a person abides to a premium percentage of 27%, what is the minimal applicable franchise-value? Keep the answer below 50 words.\"\n",
    "response = gpt_4o_mini(user_message=query,system_message=SYSTEM)\n",
    "print(f\"Response: {response.content}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "626b9083",
   "metadata": {},
   "source": [
    "### 2.2  File looping, asking the question seperately per firm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "5ef75c4a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File: WET OP LOONBELASTING 1964 \n",
      "Response: The minimal applicable franchise value, given a premium percentage of 27%, is €18,475, as this is the base amount specified. However, lower amounts may be applicable under specific conditions set by regulations, but these details would require further clarification.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import glob\n",
    "for filepath in glob.glob(\"data/docs/*.txt\")[3:4]:\n",
    "    with open(filepath, \"r\", encoding=\"utf-8\") as f:\n",
    "        text = f.read()\n",
    "\n",
    "    # Get the file name without the extension\n",
    "    file_name = str(filepath).split(\"\\\\\")[-1].split(\".\")[0].upper()\n",
    "\n",
    "    # Query\n",
    "    SYSTEM = f\"\"\"The text between triple backtics is one Dutch law text, I will ask retrieval-based questions about it: ```{text}```\"\"\"\n",
    "    query = \"In case a person abides to a premium percentage of 27%, what is the minimal applicable franchise-value? Keep the answer below 50 words.\"\n",
    "    response = gpt_4o_mini(user_message=query,system_message=SYSTEM)\n",
    "    print(f\"File: {file_name} \\nResponse: {response.content}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99c8705b",
   "metadata": {},
   "source": [
    "**Expected result:** Stuffed prompt does not denote that the 18475 does not hold for different percentages.\n",
    "\n",
    "**Takeaway:** Reducing input information size improves answer accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2d28905",
   "metadata": {},
   "source": [
    "## 3  |  Function‑Calling Experiments\n",
    "\n",
    "The next cell registers a simple calculator tool:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc58771e",
   "metadata": {},
   "source": [
    "Example of many digit computation. A transformer does not 'reason', it infers the answer likely to be the case form a test-corpus."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08e35fb0",
   "metadata": {},
   "source": [
    "### 3.1. Asking ChatGPT directly, incorrect!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "9d2fe52c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "466240730.\n"
     ]
    }
   ],
   "source": [
    "import importlib, utils\n",
    "importlib.reload(utils)\n",
    "from utils import gpt_4o_mini\n",
    "\n",
    "# Wrong answer:\n",
    "print(gpt_4o_mini(\"What is 12534 x 37245?, return me only the answer, no seperators\").content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "709470ad",
   "metadata": {},
   "source": [
    "### 3.2. Using a tool call, infers arguments from the prompt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "bd6af664",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "466828830\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import importlib, utils\n",
    "importlib.reload(utils)\n",
    "from utils import gpt_4o_mini, tool_schema\n",
    "import json\n",
    "\n",
    "#Define tool_schema for the function multiplier.\n",
    "tool_schema = {\n",
    "    \"type\": \"function\",\n",
    "    \"function\": {\n",
    "        \"name\": \"multiplier\",\n",
    "        \"parameters\": {\n",
    "            \"type\": \"object\",\n",
    "            \"description\": \"Multiply two numbers.\",\n",
    "            \"properties\": {\n",
    "                \"x\": {\n",
    "                    \"type\": \"integer\",\n",
    "                    \"description\": \"First number to multiply, must be integer or float.\",\n",
    "                },\n",
    "                \"y\": {\n",
    "                    \"type\": \"integer\",\n",
    "                    \"description\": \"First number to multiply, must be integer or float.\",\n",
    "                },\n",
    "            },\n",
    "            \"required\": [\"x\", \"y\"],\n",
    "        },\n",
    "    },\n",
    "}\n",
    "\n",
    "#Call arguments.\n",
    "arguments = json.loads(gpt_4o_mini(\"What is 12534 x 37245?\",tool_schema=tool_schema).tool_calls[0].function.arguments)\n",
    "\n",
    "#Calculate answer using the arguments.\n",
    "print(arguments[\"x\"]*arguments[\"y\"])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0806d76a",
   "metadata": {},
   "source": [
    "**Expected result:** Inaccurate computation when asking ChatGPT directly.\n",
    "\n",
    "**Takeaway:** Questions with exact answers should be answered using a tool-call."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56aa4c01",
   "metadata": {},
   "source": [
    "## 4  |  Intro to Embeddings\n",
    "\n",
    "### 4.1  Text Embeddings & Cosine Similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb10bb42",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "all-MiniLM-L6-v2\n",
      "tensor(0.7882)\n",
      "tensor(0.8582)\n",
      "tensor(0.8805)\n"
     ]
    }
   ],
   "source": [
    "from sentence_transformers import SentenceTransformer, util\n",
    "for model in [\"all-MiniLM-L6-v2\"]:\n",
    "    print(model)\n",
    "\n",
    "    #Baseline similarity\n",
    "    sim = util.cos_sim(SentenceTransformer(model).encode(\"cat\"), SentenceTransformer(model).encode(\"kitten\"))\n",
    "    print(sim[0][0])\n",
    "\n",
    "    #Decreasing attention for word that differs\n",
    "    sim = util.cos_sim(SentenceTransformer(model).encode(\"cat falls from the sky\"), SentenceTransformer(model).encode(\"kitten falls from the sky\"))\n",
    "    print(sim[0][0])\n",
    "\n",
    "    #Decreasing attention for word that differs\n",
    "    sim = util.cos_sim(SentenceTransformer(model).encode(\"cat falls from the sky and lands hard on the ground\"), SentenceTransformer(model).encode(\"kitten falls from the sky and lands hard on the ground\"))\n",
    "    print(sim[0][0])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d6ff74f",
   "metadata": {},
   "source": [
    "Expect similarity around **0.8 – 0.9** → words are semantically close.\n",
    "\n",
    "### 4.2  Text Embeddings: Good vs Bad Models\n",
    "\n",
    "| Model | |\n",
    "|-------|---------------------------------------------|\n",
    "| **`all-MiniLM-L6-v2`** | **≈ 0.80** (good) |\n",
    "| **`...`** | **≈ 0.20** (bad) |\n",
    "\n",
    "Some “general” models optimise for *token prediction*, not semantic retrieval."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ceabd802",
   "metadata": {},
   "source": [
    "## 5  |  Sentence embeddings and vector-store using Langchain"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c2d3f95",
   "metadata": {},
   "source": [
    "*Tip:* Keep the index serialized to disk so you can reuse it between sessions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63695277",
   "metadata": {},
   "source": [
    "## 6  |  LangChain RAG Pipeline (Working Demo)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f56467a3",
   "metadata": {},
   "source": [
    "*Outcome:* GPT first fetches the top‑K relevant chunks, then answers with citations."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e939c7d4",
   "metadata": {},
   "source": [
    "## 7  |  Scaling Experiments\n",
    "\n",
    "### 7.1  “Brute‑Force” Paragraph Loop (Again)\n",
    "Run against all 50 docs → ~30 s, 50 API calls.\n",
    "\n",
    "### 7.2  RAG: Filter ➜ Generate\n",
    "\n",
    "1. **Retrieve `k=4`** docs (~0.9 s)  \n",
    "2. Loop only their paragraphs (~2 s total)\n",
    "\n",
    "| Method | Latency | Tokens billed | Accuracy |\n",
    "|--------|---------|---------------|----------|\n",
    "| Brute loop | 30 s | 20 k | ✅ |\n",
    "| RAG filter+loop | **2 s** | **5 k** | ✅ |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7950fd65",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
