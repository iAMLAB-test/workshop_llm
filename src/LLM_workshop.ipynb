{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ccad53b1",
   "metadata": {},
   "source": [
    "# LLM Training Notebook\n",
    "\n",
    "This notebook accompanies an internal training session on Utilizing Large Language Models (LLMs).\n",
    "\n",
    "## Objectives\n",
    "\n",
    "By the end of this session, you should understand:\n",
    "- How to call LLMs programmatically using different frameworks (e.g., OpenAI, HuggingFace, local models).\n",
    "- The structure of responses returned by LLM APIs.\n",
    "- Best practices for prompt construction.\n",
    "- Prompt templating.\n",
    "- How function calling works with modern LLM APIs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "680175ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "\n",
    "# Install requirements\n",
    "%pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "68a13b15",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import yaml\n",
    "\n",
    "# Import \\\n",
    "parent_dir = os.path.abspath(os.path.join(os.path.dirname(__name__), \"..\"))\n",
    "config_path = os.path.join(parent_dir, \"config.yaml\")\n",
    "with open(config_path, \"r\", encoding=\"utf-8\") as f:\n",
    "    config = yaml.safe_load(f)\n",
    "\n",
    "# Set environment variables\n",
    "os.environ[\"OPENAI_API_VERSION\"] = config[\"API_VERSION_ENCODER\"]\n",
    "os.environ[\"AZURE_OPENAI_API_KEY\"] = config[\"API_KEY_ENCODER\"]\n",
    "os.environ[\"AZURE_OPENAI_API_ENDPOINT\"] = config[\"AZURE_ENDPOINT\"]\n",
    "os.environ[\"AZURE_OPENAI_MODEL_NAME\"] = config[\"API_DEPLOYMENT_NAME\"]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c3e3fa1",
   "metadata": {},
   "source": [
    "## Calling LLMs\n",
    "\n",
    "In this section, we will learn how to programmatically call Large Language Models (LLMs) using `python`.\n",
    "\n",
    "There are multiple ways to do this, but the most straightforward method is using the `openai` package to interact with a hosted LLM provided by a cloud platform such as **Azure OpenAI** or **OpenAI** directly.\n",
    "\n",
    "Most LLM APIs follow a standardized **chat-based interface**, in which messages are exchanged using roles:\n",
    "- `system`: provides initial instructions or context to the model (e.g., \"You are a helpful assistant.\")\n",
    "- `user`: represents queries or instructions from the user.\n",
    "- `assistant`: responses generated by the LLM (only used in multi-turn history).\n",
    "\n",
    "This role-based structure helps the model interpret intent and maintain context.\n",
    "\n",
    "### 2.1 OpenAI (Hosted on Azure)\n",
    "\n",
    "Azure OpenAI provides enterprise-grade access to OpenAI models, including GPT-3.5 and GPT-4, with additional benefits like regional hosting, RBAC, and cost management.\n",
    "\n",
    "#### Step 1: Deploy a model on Azure\n",
    "\n",
    "Before you can use an Azure-hosted model, you need to:\n",
    "1. Create an Azure OpenAI resource.\n",
    "2. Deploy a model (e.g., `gpt-35-turbo`, `gpt-4`) within that resource.\n",
    "3. Note the following values from your deployment:\n",
    "   - **Endpoint URI** (e.g., `https://your-resource.openai.azure.com/`)\n",
    "   - **API Key**\n",
    "   - **Deployment Name** (the identifier you gave your model)\n",
    "   - **API Version** (e.g., `2024-03-01-preview`)\n",
    "\n",
    "Using these settings in the `config_template.yaml`, we can now generate a client to have a back-and-forth with an LLM on Azure.\n",
    "\n",
    "#### Step 2: Generate a client and test the connection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "0929816d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Arrr, why did the pirate go to the gym? \n",
      "\n",
      "To improve his \"arrrr-ch!\" \n"
     ]
    }
   ],
   "source": [
    "from openai import AzureOpenAI\n",
    "\n",
    "# gets the API Key from environment variable AZURE_OPENAI_API_KEY\n",
    "client = AzureOpenAI(\n",
    "    api_key=os.getenv(\"AZURE_OPENAI_API_KEY\") or \"YOUR_API_KEY\",\n",
    "    azure_endpoint=os.getenv(\"AZURE_OPENAI_API_ENDPOINT\") or \"https://your-endpoint.openai.azure.com/\",\n",
    ")\n",
    "\n",
    "completion = client.chat.completions.create(\n",
    "    # Name we gave the deployment in Azure.\n",
    "    model=os.getenv(\"AZURE_OPENAI_MODEL_NAME\") or \"gpt-4o-mini\",\n",
    "    # The full list of messages to send to the model.\n",
    "    messages=[\n",
    "        {\"role\": \"system\", \"content\": \"You will talk like a pirate.\"},\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": \"Tell me a joke.\",\n",
    "        },\n",
    "    ],\n",
    ")\n",
    "\n",
    "# Print response\n",
    "print(completion.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee51eb86",
   "metadata": {},
   "source": [
    "#### Step 3: Let's take a look at the output structure\n",
    "\n",
    "Now that we have a `completion`, we can investigate what was retrieved from the API call.\n",
    "\n",
    "The `completion` object returned by `client.chat.completions.create` is a Python dictionary with a standardized structure. Here’s how you can inspect it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "96043e32",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ChatCompletion(id='chatcmpl-BU8dh2DGwqpmaySmpuCnvMGanWc8y', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='Arrr, why did the pirate go to the gym? \\n\\nTo improve his \"arrrr-ch!\" ', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None), content_filter_results={'hate': {'filtered': False, 'severity': 'safe'}, 'self_harm': {'filtered': False, 'severity': 'safe'}, 'sexual': {'filtered': False, 'severity': 'safe'}, 'violence': {'filtered': False, 'severity': 'safe'}})], created=1746522749, model='gpt-4o-mini-2024-07-18', object='chat.completion', service_tier=None, system_fingerprint='fp_7a53abb7a2', usage=CompletionUsage(completion_tokens=24, prompt_tokens=23, total_tokens=47, completion_tokens_details=CompletionTokensDetails(accepted_prediction_tokens=0, audio_tokens=0, reasoning_tokens=0, rejected_prediction_tokens=0), prompt_tokens_details=PromptTokensDetails(audio_tokens=0, cached_tokens=0)), prompt_filter_results=[{'prompt_index': 0, 'content_filter_results': {'hate': {'filtered': False, 'severity': 'safe'}, 'jailbreak': {'filtered': False, 'detected': False}, 'self_harm': {'filtered': False, 'severity': 'safe'}, 'sexual': {'filtered': False, 'severity': 'safe'}, 'violence': {'filtered': False, 'severity': 'safe'}}}])\n"
     ]
    }
   ],
   "source": [
    "import pprint\n",
    "pprint.pprint(completion, indent=2, width=80, compact=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26657564",
   "metadata": {},
   "source": [
    "Key fields to note:\n",
    "- choices: A list of one or more completion options. Each contains:\n",
    "- index: Position of this choice in the list.\n",
    "- message: The actual response, with role and content.\n",
    "- finish_reason: Indicates why the model stopped generating (e.g., \"stop\", \"length\", or \"function_call\").\n",
    "- usage: Token counts for prompt and completion. Useful for monitoring usage and cost.\n",
    "- id, created, model: Metadata about the request.\n",
    "\n",
    "Understanding this structure is essential when post-processing responses, logging, or chaining prompts.\n",
    "\n",
    "## Best practices - Prompt structure\n",
    "\n",
    "Writing effective prompts is one of the most important skills when working with LLMs. The quality, clarity, and intent of your prompt heavily influence the model's output.\n",
    "\n",
    "While specific models (e.g., OpenAI's `gpt-4` vs. open-source LLMs) may benefit from tailored prompt engineering, there are general best practices that apply broadly.\n",
    "\n",
    "Let’s walk through these using an example task: **asking an LLM to perform a code review of a small function**.\n",
    "\n",
    "Our initial naive prompt is `Review my code`.\n",
    "This is too vague and will likely result in a generic or superficial response.\n",
    "\n",
    "### 1. Be explicit and specific\n",
    "\n",
    "Give the model enough context to interpret the request meaningfully.\n",
    "\n",
    "**Improved prompt:**\n",
    "```text\n",
    "You are an expert senior full-stack developer. You must perform an in-depth code review of the following Python function, focusing on correctness, best practices, computational efficiency, and clarity. Do not rewrite the code; only comment where improvements can be made.\n",
    "```\n",
    "\n",
    "### 2. Define output structure\n",
    "\n",
    "Explicitly define the format of the output to ensure it's consistent and useful for downstream consumption or integration.\n",
    "\n",
    "```text\n",
    "Respond using the following structure:\n",
    "- Summary: A concise overview of the function’s quality.\n",
    "- Strengths: A bullet list of well-implemented aspects.\n",
    "- Suggestions: A bullet list of potential improvements, each with justification.\n",
    "- Severity: Indicate impact level for each suggestion (Low / Medium / High).\n",
    "```\n",
    "\n",
    "### 3. Use Constraints\n",
    "\n",
    "Control verbosity, structure, tone, or language explicitly.\n",
    "\n",
    "```text\n",
    "Limit the answer to 100 words and avoid technical jargon.\n",
    "Respond in Dutch.\n",
    "Use only Markdown for formatting.\n",
    "```\n",
    "\n",
    "### 4. Ask for Chain-Of-Thought (CoT)\n",
    "\n",
    "Encourage the model to reason explicitly before answering. This is particularly useful in complex problem-solving and debugging tasks.\n",
    "\n",
    "```text\n",
    "Think step-by-step before proposing improvements. Explain your reasoning as if teaching a junior developer.\n",
    "```\n",
    "\n",
    "### 5. Set Success Criteria\n",
    "\n",
    "Tell the model what a \"good\" response looks like.\n",
    "\n",
    "```text\n",
    "The ideal output is clear, concise, and actionable. It should be easily understood by a mid-level software engineer.\n",
    "```\n",
    "\n",
    "### 6. Use Few-shot Examples to Set Expectations\n",
    "\n",
    "Provide 1–2 examples of inputs and desired outputs before your actual task.\n",
    "This technique can dramatically improve performance on repetitive tasks or formatting-sensitive outputs.\n",
    "\n",
    "\n",
    "### 7. Chain Prompts When Needed\n",
    "\n",
    "Split complex workflows into smaller, manageable prompts. LLMs are generally more accurate when dealing with a narrow, well-defined scope.\n",
    "\n",
    "Example workflow:\n",
    "1.\tFirst prompt: Extract and summarize function purpose.\n",
    "2.\tSecond prompt: Identify inefficiencies or bugs.\n",
    "3.\tThird prompt: Suggest performance improvements."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75f85adc",
   "metadata": {},
   "source": [
    "## Final prompt example\n",
    "\n",
    "```text\n",
    "You are an expert senior full-stack developer. You must perform an in-depth code review of the following Python function, focusing on correctness, best practices, computational efficiency, and clarity.\n",
    "\n",
    "Focus on:\n",
    "- Correctness\n",
    "- Computational efficiency\n",
    "- Pythonic best practices\n",
    "- Code clarity and maintainability\n",
    "\n",
    "Constraints:\n",
    "- Do not rewrite the code.\n",
    "- Use only Markdown formatting.\n",
    "- Limit your response to 200 words.\n",
    "- Use clear, professional language.\n",
    "\n",
    "Respond using this structure:\n",
    "- **Summary**: One or two sentences summarizing the review.\n",
    "- **Strengths**: Bullet list of good practices observed.\n",
    "- **Suggestions**: Bullet list of improvements, each with a brief justification.\n",
    "- **Severity**: Label each suggestion as Low / Medium / High.\n",
    "\n",
    "The goal is to produce feedback that is clear, actionable, and helpful for a mid-level developer.\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1797b8f7",
   "metadata": {},
   "source": [
    "## Prompt Templating\n",
    "\n",
    "For general prompts that need to be reused across multiple inputs or tasks, **prompt templating** is a powerful technique. It allows you to define a reusable prompt structure and dynamically insert task-specific input at runtime. Prompt templating is fundamental for scalable LLM workflows—whether you're building chatbots, pipelines, or tools that generate or analyze structured text.\n",
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9e0b305",
   "metadata": {},
   "source": [
    "### Example: Code Review Prompt Template\n",
    "\n",
    "Suppose you want to review multiple code snippets using the same prompt pattern. You can define a template with placeholders and fill it dynamically.\n",
    "For instance, we define two placeholders: `n_words` and `code` which we fill afterwards."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "b613b8e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**Summary**: The function correctly computes the sum of a list of numbers, but there are opportunities to improve efficiency and clarity.\n",
      "\n",
      "**Strengths**:\n",
      "- Clearly defined function with appropriate naming.\n",
      "- Simple and straightforward logic for summation.\n",
      "- Properly returns the calculated total.\n",
      "\n",
      "**Suggestions**:\n",
      "- Use `sum(numbers)` instead of a loop for improved efficiency and conciseness. (High)\n",
      "- Add type hints to the function signature for better readability and maintainability. (Medium)\n",
      "- Include a docstring explaining the function's behavior and parameters. (Medium)\n",
      "\n",
      "**Severity**: High (1), Medium (2)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "prompt_template = \"\"\"\n",
    "    You are a senior full-stack engineer specializing in Python. Perform an in-depth code review of the following function.\n",
    "\n",
    "    Focus on:\n",
    "    - Correctness\n",
    "    - Computational efficiency\n",
    "    - Pythonic best practices\n",
    "    - Code clarity and maintainability\n",
    "\n",
    "    Constraints:\n",
    "    - Do not rewrite the code.\n",
    "    - Use only Markdown formatting.\n",
    "    - Limit your response to {n_words} words.\n",
    "    - Use clear, professional language.\n",
    "\n",
    "    Respond using this structure:\n",
    "    - **Summary**: One or two sentences summarizing the review.\n",
    "    - **Strengths**: Bullet list of good practices observed.\n",
    "    - **Suggestions**: Bullet list of improvements, each with a brief justification.\n",
    "    - **Severity**: Label each suggestion as Low / Medium / High.\n",
    "\n",
    "    Code:\n",
    "    ```python\n",
    "    {code}\n",
    "    ```\n",
    "\"\"\"\n",
    "\n",
    "## Use the prompt template and fill with example code and n_words\n",
    "n_words = 100\n",
    "code = \"\"\"\n",
    "def calculate_sum(numbers):\n",
    "    total = 0\n",
    "    for number in numbers:\n",
    "        total += number\n",
    "    return total\n",
    "\"\"\"\n",
    "\n",
    "prompt_filled = prompt_template.format(n_words=n_words, code=code)\n",
    "\n",
    "# Create a chat completion using the prompt\n",
    "completion = client.chat.completions.create(\n",
    "    model=os.getenv(\"AZURE_OPENAI_MODEL_NAME\") or \"gpt-4o\",\n",
    "    messages=[\n",
    "        {\"role\": \"system\", \"content\": \"You are a senior full-stack engineer specializing in Python.\"},\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": prompt_filled,\n",
    "        },\n",
    "    ],\n",
    ")\n",
    "\n",
    "# Print response\n",
    "print(completion.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "800242a4",
   "metadata": {},
   "source": [
    "## Function Calling\n",
    "\n",
    "Not all tasks should be delegated to an LLM—especially simple, deterministic operations like summing numbers or counting the occurrence of a specific character in a string. While LLMs are powerful, they are not guaranteed to be accurate on arithmetic or token-level operations due to their probabilistic nature.\n",
    "\n",
    "Instead, we can combine the strengths of LLMs with traditional code by using **function calling**.\n",
    "\n",
    "### Concept\n",
    "\n",
    "Use the LLM to interpret the user's intent and decide *which* function to call, but let actual logic and computation be handled by explicitly defined code.\n",
    "\n",
    "This pattern is especially useful in:\n",
    "\n",
    "- Natural language interfaces\n",
    "- Assistants and agents\n",
    "- Chatbots that need reliable outputs for certain tasks\n",
    "\n",
    "### Example: Count letter occurrences\n",
    "\n",
    "Let’s say the user asks:\n",
    "How many times does the letter ‘r’ appear in the word ‘strawberrry’?\n",
    "> (Note: the word contains a typo with **7 r’s**, not 3.)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "1add7542",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 5 'r's in the text \"strrrawberrrry\".\n"
     ]
    }
   ],
   "source": [
    "completion = client.chat.completions.create(\n",
    "    model=os.getenv(\"AZURE_OPENAI_MODEL_NAME\") or \"gpt-4o\",\n",
    "    messages=[\n",
    "        {\"role\": \"system\", \"content\": \"Count the number of r's in the text.\"},\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": \"strrrawberrrry\",\n",
    "        },\n",
    "    ],\n",
    ")\n",
    "\n",
    "print(completion.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a807b8bb",
   "metadata": {},
   "source": [
    "\n",
    "Rather than trusting the LLM to count letters directly, we define a function to do this reliably and we will provide the model with additional context into what that function does."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "5006f2a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model's response:\n",
      "Choice(finish_reason='tool_calls', index=0, logprobs=None, message=ChatCompletionMessage(content=None, refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=[ChatCompletionMessageToolCall(id='call_DTcxxLWqgiEJhA6xQkAjqKRE', function=Function(arguments='{\"word\":\"strrrawberrrry\",\"letter\":\"r\"}', name='count_letter'), type='function')]), content_filter_results={})\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "\n",
    "# --- Function we want to call ---\n",
    "def count_letter(word: str, letter: str) -> int:\n",
    "    return word.count(letter)\n",
    "\n",
    "\n",
    "# --- Function schema (OpenAI format) ---\n",
    "\n",
    "# --- Define the function schema using the new tools format ---\n",
    "tools = [\n",
    "    {\n",
    "        \"type\": \"function\",\n",
    "        \"function\": {\n",
    "            \"name\": \"count_letter\",\n",
    "            \"description\": \"Count how many times a letter appears in a word.\",\n",
    "            \"parameters\": {\n",
    "                \"type\": \"object\",\n",
    "                \"properties\": {\n",
    "                    \"word\": {\n",
    "                        \"type\": \"string\",\n",
    "                        \"description\": \"The word to inspect\"\n",
    "                    },\n",
    "                    \"letter\": {\n",
    "                        \"type\": \"string\",\n",
    "                        \"description\": \"The single character to count\"\n",
    "                    }\n",
    "                },\n",
    "                \"required\": [\"word\", \"letter\"]\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "]\n",
    "\n",
    "\n",
    "# --- Call the model with function-calling enabled ---\n",
    "completion = client.chat.completions.create(\n",
    "    model=os.getenv(\"AZURE_OPENAI_MODEL_NAME\") or \"gpt-4o\",\n",
    "    messages=[\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": \"How many times does the letter 'r' appear in the word 'strrrawberrrry'?\",\n",
    "        }\n",
    "    ],\n",
    "    # We add the function schema to the request\n",
    "    tools=tools,\n",
    "    tool_choice=\"auto\",\n",
    ")\n",
    "\n",
    "# Process the model's response\n",
    "choice = completion.choices[0]\n",
    "\n",
    "print(\"Model's response:\")\n",
    "print(choice)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "ea761bd1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The letter 'r' appears 7 times in 'strrrawberrrry'.\n"
     ]
    }
   ],
   "source": [
    "# Now we can call the function that the LLM thinks we should call.\n",
    "\n",
    "tool_call = choice.message.tool_calls[0]\n",
    "func_name = tool_call.function.name\n",
    "args = json.loads(tool_call.function.arguments)\n",
    "\n",
    "# Call the function in your code\n",
    "if func_name == \"count_letter\":\n",
    "    result = count_letter(**args)\n",
    "    print(f\"The letter '{args['letter']}' appears {result} times in '{args['word']}'.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32102f93",
   "metadata": {},
   "source": [
    "## Self-hosted LLM\n",
    "\n",
    "Depending on your setup, choose one of the following:\n",
    "\n",
    "### Option 1: [Ollama](https://ollama.com/) (simplest for beginners)\n",
    "\n",
    "#### Installation\n",
    "\n",
    "Install Ollama:\n",
    "\n",
    "##### Linux\n",
    "```bash\n",
    "curl -fsSL https://ollama.com/install.sh | sh\n",
    "```\n",
    "\n",
    "##### Windows\n",
    "\n",
    "Follow the instructions on the website: https://ollama.com/download/windows\n",
    "\n",
    "#### Downloading a model\n",
    "\n",
    "Search through models on https://ollama.com/search.\n",
    "\n",
    "Download a model (e.g. latest gemma with 1b parameters) using:\n",
    "\n",
    "```sh\n",
    "ollama pull gemma3:1b\n",
    "```\n",
    "\n",
    "#### Interact with Python\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "05e33758",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Why don't scientists trust atoms? \n",
      "\n",
      "Because they make up everything! \n",
      "\n",
      "---\n",
      "\n",
      "---\n",
      "\n",
      "Hope that made you smile! 😊 😊 😊 \n",
      "\n",
      "Want to hear another one?\n"
     ]
    }
   ],
   "source": [
    "import ollama\n",
    "\n",
    "response = ollama.chat(\n",
    "    model=\"gemma3:1b\",\n",
    "    messages=[{\"role\": \"user\", \"content\": \"Tell me a joke!\"}]\n",
    ")\n",
    "\n",
    "print(response['message']['content'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03cdde2e",
   "metadata": {},
   "source": [
    "### Option 2: Custom Server with Hugging Face Transformers\n",
    "\n",
    "We can also locally load and interact with models acquired from HuggingFace.\n",
    "Some models require an account and for you to accept the Terms and Conditions, but in this example we will download the `Qwen/Qwen3-1.7B` model.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "8db2115b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d59a9444bda84e7291df76a3c5584a7b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Thinking content: <think>\n",
      "Okay, the user wants a short introduction to large language models. Let me start by recalling what I know. Large language models (LLMs) are AI systems trained on vast amounts of text data. They can understand and generate human-like text. I should mention their training process, like using deep learning techniques and neural networks. Also, key features like natural language understanding and generation. Maybe touch on their applications, like in customer service or content creation. Need to keep it concise but informative. Avoid jargon so it's accessible. Check if there's anything else important, like their limitations or the difference from other models. But since it's a short intro, maybe stick to the basics. Make sure to highlight the significance of LLMs in modern technology. Alright, structure: define LLMs, mention training, key capabilities, applications, and maybe a note on their impact. Keep sentences simple and clear.\n",
      "</think>\n",
      "Content: Large language models (LLMs) are advanced AI systems designed to understand, generate, and interact with human language. Trained on vast datasets of text, they excel at tasks like answering questions, writing content, and generating code. By leveraging deep learning and neural networks, LLMs mimic human cognition, enabling them to grasp context, adapt to new information, and produce coherent, contextually relevant responses. These models power tools across industries, from customer service to creative writing, revolutionizing how we interact with technology.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "model_name = \"Qwen/Qwen3-1.7B\"\n",
    "\n",
    "# load the tokenizer and the model\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    torch_dtype=\"auto\",\n",
    "    device_map=\"auto\"\n",
    ")\n",
    "\n",
    "# prepare the model input\n",
    "prompt = \"Give me a short introduction to large language model.\"\n",
    "messages = [\n",
    "    {\"role\": \"user\", \"content\": prompt}\n",
    "]\n",
    "text = tokenizer.apply_chat_template(\n",
    "    messages,\n",
    "    tokenize=False,\n",
    "    add_generation_prompt=True,\n",
    "    enable_thinking=True # Switches between thinking and non-thinking modes. Default is True.\n",
    ")\n",
    "model_inputs = tokenizer([text], return_tensors=\"pt\").to(model.device)\n",
    "\n",
    "# conduct text completion\n",
    "generated_ids = model.generate(\n",
    "    **model_inputs,\n",
    "    max_new_tokens=32768\n",
    ")\n",
    "output_ids = generated_ids[0][len(model_inputs.input_ids[0]):].tolist() \n",
    "\n",
    "# parsing thinking content\n",
    "try:\n",
    "    # rindex finding 151668 (</think>)\n",
    "    index = len(output_ids) - output_ids[::-1].index(151668)\n",
    "except ValueError:\n",
    "    index = 0\n",
    "\n",
    "thinking_content = tokenizer.decode(output_ids[:index], skip_special_tokens=True).strip(\"\\n\")\n",
    "content = tokenizer.decode(output_ids[index:], skip_special_tokens=True).strip(\"\\n\")\n",
    "\n",
    "print(\"Thinking content:\", thinking_content)\n",
    "print(\"Content:\", content)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
