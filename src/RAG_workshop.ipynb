{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "69e56202",
   "metadata": {},
   "source": [
    "# RAG Workshop – Dutch Pension Law  \n",
    "Author: *Generated on 2025-04-30*  \n",
    "\n",
    "This notebook accompanies an internal workshop on **Retrieval‑Augmented Generation (RAG)** with Dutch pension‑related law texts from [wetten.overheid.nl](https://wetten.overheid.nl).  \n",
    "You will need:\n",
    "\n",
    "* Python 3.10+  \n",
    "* `openai`, `azure‑identity`, `langchain`, `langchain‑community`, `langchain‑huggingface`,`langchain_openai`, `tiktoken`, `rdflib`, `faiss-cpu`,`numpy`\n",
    "\n",
    "Using `config_template.yaml` fill it's contexts (provided seperately) and save under `config.yaml`, necessary to run gpt models:\n",
    "\n",
    "1. **Configure environment**\n",
    "   ```bash\n",
    "   # This is a template configuration file for the Azure OpenAI API.\n",
    "   # Fill in the placeholders with your actual configuration values.\n",
    "   # Then save the file as 'config.yaml' in the same directory.\n",
    "   AZURE_ENDPOINT: [Enter your Azure endpoint here]\n",
    "\n",
    "   ## Decoder\n",
    "   API_VERSION_DECODER: [Enter your API version here]\n",
    "   API_KEY_DECODER: [Enter your API key here]\n",
    "\n",
    "   ## Encoder\n",
    "   API_VERSION_ENCODER: [Enter your API version here]\n",
    "   API_KEY_ENCODER: [Enter your API key here]\n",
    "   ```\n",
    "2. Next run the cell below to install dependencies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfd88990",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d3c1e4d",
   "metadata": {},
   "source": [
    "### Simple LLM call."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1039e6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Prompting API example.\n",
    "from utils import gpt_4o_mini\n",
    "\n",
    "query = \"Say 'Hello world' in five different languages.\"\n",
    "print(gpt_4o_mini(query).choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81016e16",
   "metadata": {},
   "source": [
    "## 1 – Layout of Dutch law texts  \n",
    "We will inspect local `.txt` dumps in `data/docs/`. Each file contains **one complete law** (some > 60 k tokens).\n",
    "\n",
    "Below we load the all three files and print a *single article* per file to understand the structure."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f16bfcf",
   "metadata": {},
   "source": [
    "### Random article from input law texts.\n",
    "\n",
    "Here we show, using the three law texts, one random article from each."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d70b85b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import random\n",
    "from utils import split_articles, wrap_at_spaces\n",
    "\n",
    "# Load a law text file.\n",
    "DOC_PATH = Path(\"data/docs\")\n",
    "for law_file in sorted(DOC_PATH.glob(\"*.txt\")):\n",
    "    law_text = law_file.read_text(encoding=\"utf-8\")\n",
    "    articles = split_articles(law_text)  # drop preamble\n",
    "\n",
    "    example_idx = random.randrange(len(articles))\n",
    "    article_key = list(articles.keys())[example_idx]\n",
    "    print(f\"\\n--- {law_file.name}, total articles {len(articles)} ---\")\n",
    "    print(article_key)\n",
    "    print(wrap_at_spaces(articles[article_key],width=100))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ddb1b6e",
   "metadata": {},
   "source": [
    "### Count tokens per file.\n",
    "\n",
    "Given our working dataset, we show the number of tokens per law text below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1dbf9263",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Token count\n",
    "from utils import count_tokens_in_docs\n",
    "\n",
    "print(count_tokens_in_docs())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4280cb3d",
   "metadata": {},
   "source": [
    "## 2 – Direct LLM retrieval vs. Article‑wise retrieval  \n",
    "We compare:\n",
    "\n",
    "1. **Whole‑law prompt** – push the complete text (~60 k tokens)                                                 → costly ❌ fast ✔️ accurate ❌  \n",
    "2. **Chunked/article prompts** – iterate per article                                                            → costly ❌ slow ❌ accurate ✔️  \n",
    "3. **RAG** - use article text embeddings to identify five most relevant articles before searching per article   → cheap ✔️ fast ✔️ accurate ✔️  \n",
    "4. **Graph RAG** - Using a law text's linked-data structure                                                     → cheap ✔️ fast ✔️ accurate ✔️\n",
    "\n",
    "We demonstrates examples 3 and 4 at the end of the notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27e3fb2b",
   "metadata": {},
   "source": [
    "### 2.1. First, using 'naive' full text chunking in one large system prompt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8fb1b0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import json\n",
    "from utils import gpt_4o_mini, llm_metrics,wrap_at_spaces\n",
    "\n",
    "QUESTION = \"Wat is de franchise, welk artikel gebruik je ervoor en wat zijn uitzonderingen op de wettelijke waarde? Hou het antwoord onder de 50 woorden.\"\n",
    "\n",
    "# --- Whole law ---\n",
    "with open(\"data/docs/Wet op loonbelasting 1964.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "    law_text = f.read()\n",
    "    start = time.time()\n",
    "    response = gpt_4o_mini(user_message=QUESTION,law_text = law_text)\n",
    "\n",
    "    elapsed = time.time()-start\n",
    "    out_tokens = response.usage.completion_tokens\n",
    "    in_tokens = response.usage.prompt_tokens\n",
    "    print(f\"Q {QUESTION}\")\n",
    "    print(f\"A: {wrap_at_spaces(response.choices[0].message.content,100)}\")\n",
    "    print(f\"\\n--- Performance ---\")\n",
    "    print(json.dumps(llm_metrics(\"Hele wetstekst ingeladen\", in_tokens, out_tokens, elapsed),indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65893cad",
   "metadata": {},
   "source": [
    "#### Answer incorrect!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5764d5f1",
   "metadata": {},
   "source": [
    "### 2.2. Second, by looping per article section."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbe405be",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import json\n",
    "from utils import gpt_4o_mini,llm_metrics,wrap_at_spaces,split_articles\n",
    "\n",
    "QUESTION = \"Wat is de franchise, en wat zijn uitzonderingen op de wettelijke waarde? \"+\\\n",
    "\"Indien deze tekst hier geen expliciete informatie over geeft, antwoord met enkel 'None'. Hou het antwoord onder de 50 woorden.\"\n",
    "\n",
    "\n",
    "start = time.time()\n",
    "in_tokens,out_tokens = 0,0\n",
    "answers = []\n",
    "with open(\"data/docs/Wet op loonbelasting 1964.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "    law_text = f.read()\n",
    "    article_dict = split_articles(law_text)\n",
    "    for key, law_article_text in article_dict.items():\n",
    "        response = gpt_4o_mini(user_message=QUESTION,law_text = law_article_text)\n",
    "        in_tokens += response.usage.prompt_tokens\n",
    "        out_tokens += response.usage.completion_tokens\n",
    "        if \"None\" in str(response.choices[0].message.content):\n",
    "            continue\n",
    "        answers.append((key,response.choices[0].message.content))\n",
    "    elapsed = time.time()-start\n",
    "    llm_metrics(\"Zoeken per artikel\", in_tokens, out_tokens, elapsed)\n",
    "    for key,ans in answers:\n",
    "        print(f\"Q {QUESTION}\")\n",
    "        print(f\"A: {wrap_at_spaces(ans,100)}\")\n",
    "        print(f\"\\n--- Performance ---\")\n",
    "        print(json.dumps(llm_metrics(\"Hele wetstekst ingeladen\", in_tokens, out_tokens, elapsed),indent=2))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ff9d673",
   "metadata": {},
   "source": [
    "#### Answer is correct, but takes a long time to find (50+ seconds)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68a2b4fa",
   "metadata": {},
   "source": [
    "\n",
    "**Main observations**\n",
    "\n",
    "- The franchise value can be lower than € 18.475 given the premium percentage, this is only observed when looping over articles.\n",
    "- Looping over articles is accurate but takes a long time (50+ seconds in contrast to approximately 3 seconds). Both methods are costly, token-wise."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef616e29",
   "metadata": {},
   "source": [
    "## 3. Introduction to RAG"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9d49064",
   "metadata": {},
   "source": [
    "\n",
    "A word/sentence embedding is a numerical representation (using a vector) of it's semantic meaning. \n",
    "\n",
    "By creating vector-stores of texts first, one no longer has to rely on word-matching or brute force loop-searching to find the right article containing specific text\n",
    "by simply searching over a small subset of articles with an embedding closest to the question at hand, which is RAG in a nutshell. \n",
    "\n",
    "Cosine similarity measures the (L2) distance between two vectors, in this case word/sentence embeddings. \n",
    "Values are between 0 and 1, with values closer to 1 indicating words/sentences that are similar."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06f35043",
   "metadata": {},
   "source": [
    "### 3.1. Comparing embeddings ofwords/sentences."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d3ec2bb",
   "metadata": {},
   "source": [
    "Change the model parameters to observe differences across similarity metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55c56a6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from utils import text_embedding_3_large\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "\n",
    "# Adjustable parameters\n",
    "model = \"mxbai-embed-large-v1\" #\"mxbai-embed-large-v1\" or \"text_embedding_3_large\"\n",
    "\n",
    "assert model in [\"mxbai-embed-large-v1\",\"text_embedding_3_large\"], \"model must be either 'hf' or 'azure'\"\n",
    "mxbai_emb = HuggingFaceEmbeddings(model_name=\"mixedbread-ai/mxbai-embed-large-v1\")\n",
    "#Test one, same sentence, one different word.\n",
    "print( \"--- model:\",model,\"---\")\n",
    "text_tuple_list = list([(\"car\",\"cat\")]+[(f\"cat {sentence}\",f\"kitten {sentence}\") for sentence in [\"\",\"is a large animal\", \"is a large animal with much fur\"]])\n",
    "for t1,t2 in text_tuple_list:\n",
    "    if model == \"mxbai-embed-large-v1\":\n",
    "        encoder_response = mxbai_emb.embed_documents([t1,t2])\n",
    "        v1,v2 = (np.array(encoder_response[i]).reshape(1,-1) for i in [0,1])\n",
    "    else:\n",
    "        encoder_response = text_embedding_3_large([t1,t2])\n",
    "        v1,v2 = (np.array(encoder_response.data[i].embedding).reshape(1,-1) for i in [0,1])\n",
    "    print(f\"t1: {t1}\")\n",
    "    print(f\"t2: {t2}\")\n",
    "    print(\"distance\",np.round(cosine_similarity(v1,v2)[0,0],2))\n",
    "\n",
    "#Test two, same meaning, different words.\n",
    "print( \"---\")\n",
    "text_tuple_list = list([(\"rock\",\"object that beats scissors in rock paper scissors\"),(\"Tallest building in New York in 1931\",\"The Empire State Building\")])\n",
    "for t1,t2 in text_tuple_list:\n",
    "    if model == \"mxbai-embed-large-v1\":\n",
    "        encoder_response = mxbai_emb.embed_documents([t1,t2])\n",
    "        v1,v2 = (np.array(encoder_response[i]).reshape(1,-1) for i in [0,1])\n",
    "    else:\n",
    "        encoder_response = text_embedding_3_large([t1,t2])\n",
    "        v1,v2 = (np.array(encoder_response.data[i].embedding).reshape(1,-1) for i in [0,1])\n",
    "    print(f\"t1: {t1}\")\n",
    "    print(f\"t2: {t2}\")\n",
    "    print(\"distance\",np.round(cosine_similarity(v1,v2)[0,0],2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "caccdc5c",
   "metadata": {},
   "source": [
    "**This cell demonstrates two things:**\n",
    "\n",
    "When keeping one word different but increasing the sentence size, the differing word leads to a smaller difference between the resulting embeddings.\n",
    "\n",
    "Distance between 'similar' sentences using the 'text_embedding_3_large' model seem to be further apart compared to the HuggingFaceEmbeddings 'mxbai-embed-large-v1' model.\n",
    "\n",
    "This can be tested by adjusting the 'model' parameter above."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32857fec",
   "metadata": {},
   "source": [
    "### 3.2. Creating law article vector stores using both models (takes 3+ minutes on my developer-laptop), do not rerun."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ad8b86d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from pathlib import Path\n",
    "# from langchain_community.vectorstores import FAISS\n",
    "# from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "# from utils import AZURE_EMBEDDINGS, MXBAI_EMBEDDINGS, split_articles\n",
    "# import os\n",
    "\n",
    "# dir_name = os.getcwd()\n",
    "\n",
    "# # Save law texts seperately to a list and create the underlying vector_stores\n",
    "# DOC_PATH = Path(\"data/docs\")\n",
    "# article_list = []\n",
    "# for law_file in sorted(DOC_PATH.glob(\"*.txt\")):\n",
    "#     law_text = law_file.read_text(encoding=\"utf-8\")\n",
    "#     articles = split_articles(law_text)  # drop preamble\n",
    "#     article_list += [f\"\"\"{law_file.name} {key} {value}\"\"\" for key,value in articles.items()]\n",
    "\n",
    "# # 20000 chosen because the largest law text is 18000 characters.\n",
    "# splitter = RecursiveCharacterTextSplitter(chunk_size=20000, chunk_overlap=0)\n",
    "# documents = splitter.create_documents(article_list)\n",
    "\n",
    "# # 'Vector stores', for now not stored efficiently (using FAISS langchain).\n",
    "# mxbai_store = FAISS.from_documents(documents, MXBAI_EMBEDDINGS,normalize_L2=True)\n",
    "# mxbai_store.save_local(os.path.join(dir_name,\"data/vector_stores/mxbai-embed-large-v1_nongraph.index\"))\n",
    "# azure_store = FAISS.from_documents(documents, AZURE_EMBEDDINGS,normalize_L2=True)\n",
    "# azure_store.save_local(os.path.join(dir_name,\"data/vector_stores/text-embedding-3-large_nongraph.index\"))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "028e7bc3",
   "metadata": {},
   "source": [
    "## 4. Demonstrating RAG using article vector store."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f87da14",
   "metadata": {},
   "source": [
    "### 4.1. For the generated vector store shows 5 of the most closely matched articles to the original question."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23e8d9b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import textwrap\n",
    "from utils import MXBAI_STORE_NONGRAPH, AZURE_STORE_NONGRAPH, topk\n",
    "\n",
    "# number of articles to check\n",
    "n = 5\n",
    "\n",
    "QUESTION = \"Wat is de franchise, welk artikel gebruik je ervoor en wat zijn uitzonderingen op de wettelijke waarde? Hou het antwoord onder de 50 woorden.\"\n",
    "v1 = topk(MXBAI_STORE_NONGRAPH,QUESTION,k=n)\n",
    "v2 = topk(AZURE_STORE_NONGRAPH,QUESTION,k=n)\n",
    "\n",
    "article_list_mxbai = [tup[0].page_content for tup in v1]\n",
    "article_list_azure = [tup[0].page_content for tup in v2]\n",
    "\n",
    "# Pair them distance-first so heapq.nlargest uses the distance as the key\n",
    "dist_list = list(zip([\"mixedbread-ai/mxbai-embed-large-v1\",\"text_embedding_3_large\"],[article_list_mxbai, article_list_azure]))\n",
    "for name,article_list in dist_list:\n",
    "    print(f\"\\n--- Top {n} --- {name}\")\n",
    "    for i, article in enumerate(article_list):\n",
    "        print(f\"{i+1}.\",f\"{textwrap.shorten(article,150)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17d60006",
   "metadata": {},
   "source": [
    "#### Article 18a of 'Wet op loonbelasting 1964' is the correct article.\n",
    "The 'text_embedding_3_large' encoder ranks it first, 'mixedbread-ai/mxbai-embed-large-v1' model ranks it second."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd47ac57",
   "metadata": {},
   "source": [
    "### 4.2. Putting everything together, using mxbai encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "339e4ea9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import textwrap\n",
    "import json\n",
    "from utils import rag_executor, MXBAI_STORE_NONGRAPH\n",
    "\n",
    "start = time.time()\n",
    "QUESTION = \"Wat is de franchise, welk artikel gebruik je ervoor en wat zijn uitzonderingen op de wettelijke waarde? Hou het antwoord onder de 50 woorden.\"\n",
    "answer,top_docs,qa_chain,performance_cache = rag_executor(QUESTION,store = MXBAI_STORE_NONGRAPH)\n",
    "print(\"Q:\", QUESTION)\n",
    "print(\"A:\", answer)\n",
    "print(f\"\\n--- Top {len(top_docs)} documents ---\")\n",
    "for i,doc in enumerate(top_docs):\n",
    "    print(f\"{i+1}.\",f\"{textwrap.shorten(str(doc.page_content),150)}\")\n",
    "\n",
    "print(f\"\\n--- Performance ---\")\n",
    "in_tokens = performance_cache[\"in_tokens\"]\n",
    "out_tokens = performance_cache[\"out_tokens\"]\n",
    "elapsed = performance_cache[\"elapsed\"]\n",
    "print(json.dumps(llm_metrics(\"RAG op wetsartikelen\", in_tokens, out_tokens, elapsed),indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5c70f58",
   "metadata": {},
   "source": [
    "#### Much faster and accurate compared to the methods in Section 2.1 or 2.2."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac8d4a49",
   "metadata": {},
   "source": [
    "## 5. Graph RAG, taking advantage of linked data structure for RAG."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8831d282",
   "metadata": {},
   "source": [
    "### 5.1. Law graph (created beforehand)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c688dc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO: Placeholder, visualize graph."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ca18065",
   "metadata": {},
   "source": [
    "### 5.2. Graph-based vector store."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2605c15",
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO: Placeholder, create vector stores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "691a423d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO: Placeholder, display top 5 articles based on a query."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abf3321d",
   "metadata": {},
   "source": [
    "### 5.3. Questions across articles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b372a125",
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO: Graph RAG demonstration."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb3808b3",
   "metadata": {},
   "source": [
    "## 6. Wrap‑up / Key takeaways ✅  \n",
    "\n",
    "* **Direct prompting** on entire laws is cost‑heavy and hits context limits.  \n",
    "* **Chunking** improves alignment but sacrifices latency.  \n",
    "* **RAG** with a high‑quality embedding model (MXBAI) gives the *best accuracy‑per‑dollar*.  \n",
    "* Integrating domain‑specific knowledge graphs can further boost recall for cross‑article references.  \n",
    "\n",
    "Feel free to extend the notebook by  \n",
    "* replacing placeholder accuracies with manual grading,  \n",
    "* adding caching for embeddings,  \n",
    "* deploying the FAISS index as an API.  \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
