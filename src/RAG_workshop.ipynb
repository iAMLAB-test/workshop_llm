{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "69e56202",
   "metadata": {},
   "source": [
    "# RAG Workshop – Dutch Pension Law  \n",
    "Author: *Generated on 2025-04-30*  \n",
    "\n",
    "This notebook accompanies an internal workshop on **Retrieval‑Augmented Generation (RAG)** with Dutch pension‑related law texts from [wetten.overheid.nl](https://wetten.overheid.nl).  \n",
    "You will need:\n",
    "\n",
    "* Python 3.10+  \n",
    "* `openai`, `azure‑identity`, `langchain`, `langchain‑community`, `langchain‑huggingface`,`langchain_openai`, `tiktoken`, `rdflib`, `faiss-cpu`,`numpy`\n",
    "\n",
    "Using `config_template.yaml` fill it's contexts (provided seperately) and save under `config.yaml`, necessary to run gpt models:\n",
    "\n",
    "1. **Configure environment**\n",
    "   ```bash\n",
    "   # This is a template configuration file for the Azure OpenAI API.\n",
    "   # Fill in the placeholders with your actual configuration values.\n",
    "   # Then save the file as 'config.yaml' in the same directory.\n",
    "   AZURE_ENDPOINT: [Enter your Azure endpoint here]\n",
    "\n",
    "   ## Decoder\n",
    "   API_VERSION_DECODER: [Enter your API version here]\n",
    "   API_KEY_DECODER: [Enter your API key here]\n",
    "\n",
    "   ## Encoder\n",
    "   API_VERSION_ENCODER: [Enter your API version here]\n",
    "   API_KEY_ENCODER: [Enter your API key here]\n",
    "   ```\n",
    "2. Next run the cell below to install dependencies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "dfd88990",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: openai in c:\\users\\al.lee\\downloads\\rag_workshop\\.venv\\lib\\site-packages (from -r requirements.txt (line 1)) (1.76.2)\n",
      "Requirement already satisfied: langchain in c:\\users\\al.lee\\downloads\\rag_workshop\\.venv\\lib\\site-packages (from -r requirements.txt (line 2)) (0.3.24)\n",
      "Requirement already satisfied: langchain-community in c:\\users\\al.lee\\downloads\\rag_workshop\\.venv\\lib\\site-packages (from -r requirements.txt (line 3)) (0.3.23)\n",
      "Requirement already satisfied: langchain-huggingface in c:\\users\\al.lee\\downloads\\rag_workshop\\.venv\\lib\\site-packages (from -r requirements.txt (line 4)) (0.1.2)\n",
      "Requirement already satisfied: langchain_openai in c:\\users\\al.lee\\downloads\\rag_workshop\\.venv\\lib\\site-packages (from -r requirements.txt (line 5)) (0.3.14)\n",
      "Requirement already satisfied: langchain_core in c:\\users\\al.lee\\downloads\\rag_workshop\\.venv\\lib\\site-packages (from -r requirements.txt (line 6)) (0.3.56)\n",
      "Requirement already satisfied: tiktoken in c:\\users\\al.lee\\downloads\\rag_workshop\\.venv\\lib\\site-packages (from -r requirements.txt (line 7)) (0.9.0)\n",
      "Requirement already satisfied: rdflib in c:\\users\\al.lee\\downloads\\rag_workshop\\.venv\\lib\\site-packages (from -r requirements.txt (line 8)) (7.1.4)\n",
      "Requirement already satisfied: python-dotenv in c:\\users\\al.lee\\downloads\\rag_workshop\\.venv\\lib\\site-packages (from -r requirements.txt (line 9)) (1.1.0)\n",
      "Requirement already satisfied: faiss-cpu in c:\\users\\al.lee\\downloads\\rag_workshop\\.venv\\lib\\site-packages (from -r requirements.txt (line 10)) (1.11.0)\n",
      "Requirement already satisfied: numpy in c:\\users\\al.lee\\downloads\\rag_workshop\\.venv\\lib\\site-packages (from -r requirements.txt (line 11)) (2.2.5)\n",
      "Requirement already satisfied: scikit-learn in c:\\users\\al.lee\\downloads\\rag_workshop\\.venv\\lib\\site-packages (from -r requirements.txt (line 12)) (1.6.1)\n",
      "Requirement already satisfied: ipykernel in c:\\users\\al.lee\\downloads\\rag_workshop\\.venv\\lib\\site-packages (from -r requirements.txt (line 13)) (6.29.5)\n",
      "Requirement already satisfied: ipywidgets in c:\\users\\al.lee\\downloads\\rag_workshop\\.venv\\lib\\site-packages (from -r requirements.txt (line 14)) (8.1.6)\n",
      "Requirement already satisfied: anyio<5,>=3.5.0 in c:\\users\\al.lee\\downloads\\rag_workshop\\.venv\\lib\\site-packages (from openai->-r requirements.txt (line 1)) (4.9.0)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in c:\\users\\al.lee\\downloads\\rag_workshop\\.venv\\lib\\site-packages (from openai->-r requirements.txt (line 1)) (1.9.0)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in c:\\users\\al.lee\\downloads\\rag_workshop\\.venv\\lib\\site-packages (from openai->-r requirements.txt (line 1)) (0.28.1)\n",
      "Requirement already satisfied: jiter<1,>=0.4.0 in c:\\users\\al.lee\\downloads\\rag_workshop\\.venv\\lib\\site-packages (from openai->-r requirements.txt (line 1)) (0.9.0)\n",
      "Requirement already satisfied: pydantic<3,>=1.9.0 in c:\\users\\al.lee\\downloads\\rag_workshop\\.venv\\lib\\site-packages (from openai->-r requirements.txt (line 1)) (2.11.4)\n",
      "Requirement already satisfied: sniffio in c:\\users\\al.lee\\downloads\\rag_workshop\\.venv\\lib\\site-packages (from openai->-r requirements.txt (line 1)) (1.3.1)\n",
      "Requirement already satisfied: tqdm>4 in c:\\users\\al.lee\\downloads\\rag_workshop\\.venv\\lib\\site-packages (from openai->-r requirements.txt (line 1)) (4.67.1)\n",
      "Requirement already satisfied: typing-extensions<5,>=4.11 in c:\\users\\al.lee\\downloads\\rag_workshop\\.venv\\lib\\site-packages (from openai->-r requirements.txt (line 1)) (4.13.2)\n",
      "Requirement already satisfied: idna>=2.8 in c:\\users\\al.lee\\downloads\\rag_workshop\\.venv\\lib\\site-packages (from anyio<5,>=3.5.0->openai->-r requirements.txt (line 1)) (3.10)\n",
      "Requirement already satisfied: certifi in c:\\users\\al.lee\\downloads\\rag_workshop\\.venv\\lib\\site-packages (from httpx<1,>=0.23.0->openai->-r requirements.txt (line 1)) (2025.4.26)\n",
      "Requirement already satisfied: httpcore==1.* in c:\\users\\al.lee\\downloads\\rag_workshop\\.venv\\lib\\site-packages (from httpx<1,>=0.23.0->openai->-r requirements.txt (line 1)) (1.0.9)\n",
      "Requirement already satisfied: h11>=0.16 in c:\\users\\al.lee\\downloads\\rag_workshop\\.venv\\lib\\site-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai->-r requirements.txt (line 1)) (0.16.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in c:\\users\\al.lee\\downloads\\rag_workshop\\.venv\\lib\\site-packages (from pydantic<3,>=1.9.0->openai->-r requirements.txt (line 1)) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.33.2 in c:\\users\\al.lee\\downloads\\rag_workshop\\.venv\\lib\\site-packages (from pydantic<3,>=1.9.0->openai->-r requirements.txt (line 1)) (2.33.2)\n",
      "Requirement already satisfied: typing-inspection>=0.4.0 in c:\\users\\al.lee\\downloads\\rag_workshop\\.venv\\lib\\site-packages (from pydantic<3,>=1.9.0->openai->-r requirements.txt (line 1)) (0.4.0)\n",
      "Requirement already satisfied: langchain-text-splitters<1.0.0,>=0.3.8 in c:\\users\\al.lee\\downloads\\rag_workshop\\.venv\\lib\\site-packages (from langchain->-r requirements.txt (line 2)) (0.3.8)\n",
      "Requirement already satisfied: langsmith<0.4,>=0.1.17 in c:\\users\\al.lee\\downloads\\rag_workshop\\.venv\\lib\\site-packages (from langchain->-r requirements.txt (line 2)) (0.3.39)\n",
      "Requirement already satisfied: SQLAlchemy<3,>=1.4 in c:\\users\\al.lee\\downloads\\rag_workshop\\.venv\\lib\\site-packages (from langchain->-r requirements.txt (line 2)) (2.0.40)\n",
      "Requirement already satisfied: requests<3,>=2 in c:\\users\\al.lee\\downloads\\rag_workshop\\.venv\\lib\\site-packages (from langchain->-r requirements.txt (line 2)) (2.32.3)\n",
      "Requirement already satisfied: PyYAML>=5.3 in c:\\users\\al.lee\\downloads\\rag_workshop\\.venv\\lib\\site-packages (from langchain->-r requirements.txt (line 2)) (6.0.2)\n",
      "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.1.0 in c:\\users\\al.lee\\downloads\\rag_workshop\\.venv\\lib\\site-packages (from langchain_core->-r requirements.txt (line 6)) (9.1.2)\n",
      "Requirement already satisfied: jsonpatch<2.0,>=1.33 in c:\\users\\al.lee\\downloads\\rag_workshop\\.venv\\lib\\site-packages (from langchain_core->-r requirements.txt (line 6)) (1.33)\n",
      "Requirement already satisfied: packaging<25,>=23.2 in c:\\users\\al.lee\\downloads\\rag_workshop\\.venv\\lib\\site-packages (from langchain_core->-r requirements.txt (line 6)) (24.2)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in c:\\users\\al.lee\\downloads\\rag_workshop\\.venv\\lib\\site-packages (from jsonpatch<2.0,>=1.33->langchain_core->-r requirements.txt (line 6)) (3.0.0)\n",
      "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in c:\\users\\al.lee\\downloads\\rag_workshop\\.venv\\lib\\site-packages (from langsmith<0.4,>=0.1.17->langchain->-r requirements.txt (line 2)) (3.10.18)\n",
      "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in c:\\users\\al.lee\\downloads\\rag_workshop\\.venv\\lib\\site-packages (from langsmith<0.4,>=0.1.17->langchain->-r requirements.txt (line 2)) (1.0.0)\n",
      "Requirement already satisfied: zstandard<0.24.0,>=0.23.0 in c:\\users\\al.lee\\downloads\\rag_workshop\\.venv\\lib\\site-packages (from langsmith<0.4,>=0.1.17->langchain->-r requirements.txt (line 2)) (0.23.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\al.lee\\downloads\\rag_workshop\\.venv\\lib\\site-packages (from requests<3,>=2->langchain->-r requirements.txt (line 2)) (3.4.1)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\al.lee\\downloads\\rag_workshop\\.venv\\lib\\site-packages (from requests<3,>=2->langchain->-r requirements.txt (line 2)) (2.4.0)\n",
      "Requirement already satisfied: greenlet>=1 in c:\\users\\al.lee\\downloads\\rag_workshop\\.venv\\lib\\site-packages (from SQLAlchemy<3,>=1.4->langchain->-r requirements.txt (line 2)) (3.2.1)\n",
      "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in c:\\users\\al.lee\\downloads\\rag_workshop\\.venv\\lib\\site-packages (from langchain-community->-r requirements.txt (line 3)) (3.11.18)\n",
      "Requirement already satisfied: dataclasses-json<0.7,>=0.5.7 in c:\\users\\al.lee\\downloads\\rag_workshop\\.venv\\lib\\site-packages (from langchain-community->-r requirements.txt (line 3)) (0.6.7)\n",
      "Requirement already satisfied: pydantic-settings<3.0.0,>=2.4.0 in c:\\users\\al.lee\\downloads\\rag_workshop\\.venv\\lib\\site-packages (from langchain-community->-r requirements.txt (line 3)) (2.9.1)\n",
      "Requirement already satisfied: httpx-sse<1.0.0,>=0.4.0 in c:\\users\\al.lee\\downloads\\rag_workshop\\.venv\\lib\\site-packages (from langchain-community->-r requirements.txt (line 3)) (0.4.0)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in c:\\users\\al.lee\\downloads\\rag_workshop\\.venv\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community->-r requirements.txt (line 3)) (2.6.1)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in c:\\users\\al.lee\\downloads\\rag_workshop\\.venv\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community->-r requirements.txt (line 3)) (1.3.2)\n",
      "Requirement already satisfied: attrs>=17.3.0 in c:\\users\\al.lee\\downloads\\rag_workshop\\.venv\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community->-r requirements.txt (line 3)) (25.3.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in c:\\users\\al.lee\\downloads\\rag_workshop\\.venv\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community->-r requirements.txt (line 3)) (1.6.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in c:\\users\\al.lee\\downloads\\rag_workshop\\.venv\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community->-r requirements.txt (line 3)) (6.4.3)\n",
      "Requirement already satisfied: propcache>=0.2.0 in c:\\users\\al.lee\\downloads\\rag_workshop\\.venv\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community->-r requirements.txt (line 3)) (0.3.1)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in c:\\users\\al.lee\\downloads\\rag_workshop\\.venv\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community->-r requirements.txt (line 3)) (1.20.0)\n",
      "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in c:\\users\\al.lee\\downloads\\rag_workshop\\.venv\\lib\\site-packages (from dataclasses-json<0.7,>=0.5.7->langchain-community->-r requirements.txt (line 3)) (3.26.1)\n",
      "Requirement already satisfied: typing-inspect<1,>=0.4.0 in c:\\users\\al.lee\\downloads\\rag_workshop\\.venv\\lib\\site-packages (from dataclasses-json<0.7,>=0.5.7->langchain-community->-r requirements.txt (line 3)) (0.9.0)\n",
      "Requirement already satisfied: mypy-extensions>=0.3.0 in c:\\users\\al.lee\\downloads\\rag_workshop\\.venv\\lib\\site-packages (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain-community->-r requirements.txt (line 3)) (1.1.0)\n",
      "Requirement already satisfied: huggingface-hub>=0.23.0 in c:\\users\\al.lee\\downloads\\rag_workshop\\.venv\\lib\\site-packages (from langchain-huggingface->-r requirements.txt (line 4)) (0.30.2)\n",
      "Requirement already satisfied: sentence-transformers>=2.6.0 in c:\\users\\al.lee\\downloads\\rag_workshop\\.venv\\lib\\site-packages (from langchain-huggingface->-r requirements.txt (line 4)) (4.1.0)\n",
      "Requirement already satisfied: tokenizers>=0.19.1 in c:\\users\\al.lee\\downloads\\rag_workshop\\.venv\\lib\\site-packages (from langchain-huggingface->-r requirements.txt (line 4)) (0.21.1)\n",
      "Requirement already satisfied: transformers>=4.39.0 in c:\\users\\al.lee\\downloads\\rag_workshop\\.venv\\lib\\site-packages (from langchain-huggingface->-r requirements.txt (line 4)) (4.51.3)\n",
      "Requirement already satisfied: regex>=2022.1.18 in c:\\users\\al.lee\\downloads\\rag_workshop\\.venv\\lib\\site-packages (from tiktoken->-r requirements.txt (line 7)) (2024.11.6)\n",
      "Requirement already satisfied: pyparsing<4,>=2.1.0 in c:\\users\\al.lee\\downloads\\rag_workshop\\.venv\\lib\\site-packages (from rdflib->-r requirements.txt (line 8)) (3.2.3)\n",
      "Requirement already satisfied: scipy>=1.6.0 in c:\\users\\al.lee\\downloads\\rag_workshop\\.venv\\lib\\site-packages (from scikit-learn->-r requirements.txt (line 12)) (1.15.2)\n",
      "Requirement already satisfied: joblib>=1.2.0 in c:\\users\\al.lee\\downloads\\rag_workshop\\.venv\\lib\\site-packages (from scikit-learn->-r requirements.txt (line 12)) (1.4.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in c:\\users\\al.lee\\downloads\\rag_workshop\\.venv\\lib\\site-packages (from scikit-learn->-r requirements.txt (line 12)) (3.6.0)\n",
      "Requirement already satisfied: comm>=0.1.1 in c:\\users\\al.lee\\downloads\\rag_workshop\\.venv\\lib\\site-packages (from ipykernel->-r requirements.txt (line 13)) (0.2.2)\n",
      "Requirement already satisfied: debugpy>=1.6.5 in c:\\users\\al.lee\\downloads\\rag_workshop\\.venv\\lib\\site-packages (from ipykernel->-r requirements.txt (line 13)) (1.8.14)\n",
      "Requirement already satisfied: ipython>=7.23.1 in c:\\users\\al.lee\\downloads\\rag_workshop\\.venv\\lib\\site-packages (from ipykernel->-r requirements.txt (line 13)) (9.2.0)\n",
      "Requirement already satisfied: jupyter-client>=6.1.12 in c:\\users\\al.lee\\downloads\\rag_workshop\\.venv\\lib\\site-packages (from ipykernel->-r requirements.txt (line 13)) (8.6.3)\n",
      "Requirement already satisfied: jupyter-core!=5.0.*,>=4.12 in c:\\users\\al.lee\\downloads\\rag_workshop\\.venv\\lib\\site-packages (from ipykernel->-r requirements.txt (line 13)) (5.7.2)\n",
      "Requirement already satisfied: matplotlib-inline>=0.1 in c:\\users\\al.lee\\downloads\\rag_workshop\\.venv\\lib\\site-packages (from ipykernel->-r requirements.txt (line 13)) (0.1.7)\n",
      "Requirement already satisfied: nest-asyncio in c:\\users\\al.lee\\downloads\\rag_workshop\\.venv\\lib\\site-packages (from ipykernel->-r requirements.txt (line 13)) (1.6.0)\n",
      "Requirement already satisfied: psutil in c:\\users\\al.lee\\downloads\\rag_workshop\\.venv\\lib\\site-packages (from ipykernel->-r requirements.txt (line 13)) (7.0.0)\n",
      "Requirement already satisfied: pyzmq>=24 in c:\\users\\al.lee\\downloads\\rag_workshop\\.venv\\lib\\site-packages (from ipykernel->-r requirements.txt (line 13)) (26.4.0)\n",
      "Requirement already satisfied: tornado>=6.1 in c:\\users\\al.lee\\downloads\\rag_workshop\\.venv\\lib\\site-packages (from ipykernel->-r requirements.txt (line 13)) (6.4.2)\n",
      "Requirement already satisfied: traitlets>=5.4.0 in c:\\users\\al.lee\\downloads\\rag_workshop\\.venv\\lib\\site-packages (from ipykernel->-r requirements.txt (line 13)) (5.14.3)\n",
      "Requirement already satisfied: widgetsnbextension~=4.0.14 in c:\\users\\al.lee\\downloads\\rag_workshop\\.venv\\lib\\site-packages (from ipywidgets->-r requirements.txt (line 14)) (4.0.14)\n",
      "Requirement already satisfied: jupyterlab_widgets~=3.0.14 in c:\\users\\al.lee\\downloads\\rag_workshop\\.venv\\lib\\site-packages (from ipywidgets->-r requirements.txt (line 14)) (3.0.14)\n",
      "Requirement already satisfied: filelock in c:\\users\\al.lee\\downloads\\rag_workshop\\.venv\\lib\\site-packages (from huggingface-hub>=0.23.0->langchain-huggingface->-r requirements.txt (line 4)) (3.18.0)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in c:\\users\\al.lee\\downloads\\rag_workshop\\.venv\\lib\\site-packages (from huggingface-hub>=0.23.0->langchain-huggingface->-r requirements.txt (line 4)) (2025.3.2)\n",
      "Requirement already satisfied: colorama in c:\\users\\al.lee\\downloads\\rag_workshop\\.venv\\lib\\site-packages (from ipython>=7.23.1->ipykernel->-r requirements.txt (line 13)) (0.4.6)\n",
      "Requirement already satisfied: decorator in c:\\users\\al.lee\\downloads\\rag_workshop\\.venv\\lib\\site-packages (from ipython>=7.23.1->ipykernel->-r requirements.txt (line 13)) (5.2.1)\n",
      "Requirement already satisfied: ipython-pygments-lexers in c:\\users\\al.lee\\downloads\\rag_workshop\\.venv\\lib\\site-packages (from ipython>=7.23.1->ipykernel->-r requirements.txt (line 13)) (1.1.1)\n",
      "Requirement already satisfied: jedi>=0.16 in c:\\users\\al.lee\\downloads\\rag_workshop\\.venv\\lib\\site-packages (from ipython>=7.23.1->ipykernel->-r requirements.txt (line 13)) (0.19.2)\n",
      "Requirement already satisfied: prompt_toolkit<3.1.0,>=3.0.41 in c:\\users\\al.lee\\downloads\\rag_workshop\\.venv\\lib\\site-packages (from ipython>=7.23.1->ipykernel->-r requirements.txt (line 13)) (3.0.51)\n",
      "Requirement already satisfied: pygments>=2.4.0 in c:\\users\\al.lee\\downloads\\rag_workshop\\.venv\\lib\\site-packages (from ipython>=7.23.1->ipykernel->-r requirements.txt (line 13)) (2.19.1)\n",
      "Requirement already satisfied: stack_data in c:\\users\\al.lee\\downloads\\rag_workshop\\.venv\\lib\\site-packages (from ipython>=7.23.1->ipykernel->-r requirements.txt (line 13)) (0.6.3)\n",
      "Requirement already satisfied: wcwidth in c:\\users\\al.lee\\downloads\\rag_workshop\\.venv\\lib\\site-packages (from prompt_toolkit<3.1.0,>=3.0.41->ipython>=7.23.1->ipykernel->-r requirements.txt (line 13)) (0.2.13)\n",
      "Requirement already satisfied: parso<0.9.0,>=0.8.4 in c:\\users\\al.lee\\downloads\\rag_workshop\\.venv\\lib\\site-packages (from jedi>=0.16->ipython>=7.23.1->ipykernel->-r requirements.txt (line 13)) (0.8.4)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\al.lee\\downloads\\rag_workshop\\.venv\\lib\\site-packages (from jupyter-client>=6.1.12->ipykernel->-r requirements.txt (line 13)) (2.9.0.post0)\n",
      "Requirement already satisfied: platformdirs>=2.5 in c:\\users\\al.lee\\downloads\\rag_workshop\\.venv\\lib\\site-packages (from jupyter-core!=5.0.*,>=4.12->ipykernel->-r requirements.txt (line 13)) (4.3.7)\n",
      "Requirement already satisfied: pywin32>=300 in c:\\users\\al.lee\\downloads\\rag_workshop\\.venv\\lib\\site-packages (from jupyter-core!=5.0.*,>=4.12->ipykernel->-r requirements.txt (line 13)) (310)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\al.lee\\downloads\\rag_workshop\\.venv\\lib\\site-packages (from python-dateutil>=2.8.2->jupyter-client>=6.1.12->ipykernel->-r requirements.txt (line 13)) (1.17.0)\n",
      "Requirement already satisfied: torch>=1.11.0 in c:\\users\\al.lee\\downloads\\rag_workshop\\.venv\\lib\\site-packages (from sentence-transformers>=2.6.0->langchain-huggingface->-r requirements.txt (line 4)) (2.7.0)\n",
      "Requirement already satisfied: Pillow in c:\\users\\al.lee\\downloads\\rag_workshop\\.venv\\lib\\site-packages (from sentence-transformers>=2.6.0->langchain-huggingface->-r requirements.txt (line 4)) (11.2.1)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in c:\\users\\al.lee\\downloads\\rag_workshop\\.venv\\lib\\site-packages (from transformers>=4.39.0->langchain-huggingface->-r requirements.txt (line 4)) (0.5.3)\n",
      "Requirement already satisfied: sympy>=1.13.3 in c:\\users\\al.lee\\downloads\\rag_workshop\\.venv\\lib\\site-packages (from torch>=1.11.0->sentence-transformers>=2.6.0->langchain-huggingface->-r requirements.txt (line 4)) (1.14.0)\n",
      "Requirement already satisfied: networkx in c:\\users\\al.lee\\downloads\\rag_workshop\\.venv\\lib\\site-packages (from torch>=1.11.0->sentence-transformers>=2.6.0->langchain-huggingface->-r requirements.txt (line 4)) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\al.lee\\downloads\\rag_workshop\\.venv\\lib\\site-packages (from torch>=1.11.0->sentence-transformers>=2.6.0->langchain-huggingface->-r requirements.txt (line 4)) (3.1.6)\n",
      "Requirement already satisfied: setuptools in c:\\users\\al.lee\\downloads\\rag_workshop\\.venv\\lib\\site-packages (from torch>=1.11.0->sentence-transformers>=2.6.0->langchain-huggingface->-r requirements.txt (line 4)) (80.0.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\users\\al.lee\\downloads\\rag_workshop\\.venv\\lib\\site-packages (from sympy>=1.13.3->torch>=1.11.0->sentence-transformers>=2.6.0->langchain-huggingface->-r requirements.txt (line 4)) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\al.lee\\downloads\\rag_workshop\\.venv\\lib\\site-packages (from jinja2->torch>=1.11.0->sentence-transformers>=2.6.0->langchain-huggingface->-r requirements.txt (line 4)) (3.0.2)\n",
      "Requirement already satisfied: executing>=1.2.0 in c:\\users\\al.lee\\downloads\\rag_workshop\\.venv\\lib\\site-packages (from stack_data->ipython>=7.23.1->ipykernel->-r requirements.txt (line 13)) (2.2.0)\n",
      "Requirement already satisfied: asttokens>=2.1.0 in c:\\users\\al.lee\\downloads\\rag_workshop\\.venv\\lib\\site-packages (from stack_data->ipython>=7.23.1->ipykernel->-r requirements.txt (line 13)) (3.0.0)\n",
      "Requirement already satisfied: pure-eval in c:\\users\\al.lee\\downloads\\rag_workshop\\.venv\\lib\\site-packages (from stack_data->ipython>=7.23.1->ipykernel->-r requirements.txt (line 13)) (0.2.3)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d3c1e4d",
   "metadata": {},
   "source": [
    "### Simple LLM call."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e1039e6a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sure! Here’s \"Hello world\" in five different languages:\n",
      "\n",
      "1. English: Hello world\n",
      "2. Spanish: Hola mundo\n",
      "3. French: Bonjour le monde\n",
      "4. German: Hallo Welt\n",
      "5. Italian: Ciao mondo\n"
     ]
    }
   ],
   "source": [
    "#Prompting API example.\n",
    "from utils import gpt_4o_mini\n",
    "\n",
    "query = \"Say 'Hello world' in five different languages.\"\n",
    "print(gpt_4o_mini(query).choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81016e16",
   "metadata": {},
   "source": [
    "## 1 – Layout of Dutch law texts  \n",
    "We will inspect local `.txt` dumps in `data/docs/`. Each file contains **one complete law** (some > 60 k tokens).\n",
    "\n",
    "Below we load the all three files and print a *single article* per file to understand the structure."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f16bfcf",
   "metadata": {},
   "source": [
    "### Random article from input law texts.\n",
    "\n",
    "Here we show, using the three law texts, one random article from each."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7d70b85b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Algemene ouderdomswet.txt, total articles 110 ---\n",
      "Artikel 17b\n",
      "1.      De Sociale verzekeringsbank weigert het ouderdomspensioen geheel of gedeeltelijk, tijdelijk\n",
      "of blijvend, indien de pensioengerechtigde, zijn echtgenoot, of zijn wettelijke vertegenwoordiger\n",
      "een verplichting, hem op grond van artikel 15, tweede of derde lid, opgelegd, of de verplichtingen,\n",
      "bedoeld in artikel 55, tweede lid, van de Wet structuur uitvoeringsorganisatie werk en inkomen niet\n",
      "of niet behoorlijk is nagekomen, dan wel de verplichting, bedoeld in artikel 49 niet binnen de door\n",
      "de Sociale verzekeringsbank daarvoor vastgestelde termijn is nagekomen.  2.      Een maatregel als\n",
      "bedoeld in het eerste lid wordt afgestemd op de ernst van de gedraging en de mate waarin de\n",
      "belanghebbende de gedraging verweten kan worden. Van het opleggen van een maatregel wordt in elk\n",
      "geval afgezien, indien elke vorm van verwijtbaarheid ontbreekt.  3.      De Sociale verzekeringsbank\n",
      "kan afzien van het opleggen van een maatregel als bedoeld in het eerste lid en volstaan met het\n",
      "geven van een schriftelijke waarschuwing ter zake het niet tijdig nakomen van de verplichting,\n",
      "bedoeld in artikel 49, indien het niet tijdig nakomen van de verplichting niet heeft geleid tot het\n",
      "ten onrechte of tot een te hoog bedrag verlenen van ouderdomspensioen, tenzij het niet tijdig\n",
      "nakomen van de verplichting plaatsvindt binnen een periode van twee jaar te rekenen vanaf de datum\n",
      "waarop eerder aan de belanghebbende een zodanige waarschuwing is gegeven.  4.      De Sociale\n",
      "verzekeringsbank kan afzien van het opleggen van een maatregel indien daarvoor dringende redenen\n",
      "aanwezig zijn.  5.      Het opleggen van een maatregel blijft achterwege indien voor dezelfde\n",
      "gedraging een bestuurlijke boete als bedoeld in artikel 17c wordt opgelegd.  6.      Bij of\n",
      "krachtens algemene maatregel van bestuur worden nadere regels gesteld met betrekking tot het eerste\n",
      "en tweede lid.\n",
      "\n",
      "--- Pensioenwet.txt, total articles 323 ---\n",
      "Artikel 165.\n",
      "Aanwijzing door Onze Minister  1.      Onze Minister kan aan de toezichthouder een aanwijzing geven\n",
      "over de uitoefening van de aan de toezichthouder bij of krachtens deze wet opgedragen taken en\n",
      "toegekende bevoegdheden wanneer de toezichthouder hierin naar het oordeel van Onze Minister tekort\n",
      "schiet. Onze Minister treedt daarbij niet in individuele gevallen.  2.      De toezichthouder is\n",
      "gehouden overeenkomstig de aanwijzing te handelen.\n",
      "\n",
      "--- Uitvoeringsbesluit loonbelasting 1965.txt, total articles 50 ---\n",
      "Artikel 11a\n",
      "Ingeval een verzekeraar als bedoeld in artikel 19a van de wet overeenkomt met de inhoudingsplichtige\n",
      "die aan de werknemer loon verstrekt als bedoeld in artikel 31, vierde lid, onderdeel h, van de wet,\n",
      "dat die verzekeraar de ter zake van dat loon verschuldigde loonbelasting inhoudt op het pensioen,\n",
      "wordt niet die inhoudingsplichtige, maar die verzekeraar ter zake van dat loon als\n",
      "inhoudingsplichtige beschouwd.\n",
      "\n",
      "--- Wet op loonbelasting 1964.txt, total articles 169 ---\n",
      "Artikel 5\n",
      "1.      Als dienstbetrekking wordt niet beschouwd de arbeidsverhouding van degene die uitsluitend of\n",
      "nagenoeg uitsluitend diensten verricht ten behoeve van het huishouden van de natuurlijke persoon tot\n",
      "wie hij in dienstbetrekking staat, indien hij de diensten doorgaans op minder dan vier dagen per\n",
      "week verricht.  2.      Onder het verrichten van diensten ten behoeve van een huishouden wordt voor\n",
      "de toepassing van dit artikel mede verstaan het verlenen van zorg aan de leden van dat huishouden.\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "import random\n",
    "from utils import split_articles, wrap_at_spaces\n",
    "\n",
    "# Load a law text file.\n",
    "DOC_PATH = Path(\"data/texts\")\n",
    "for law_file in sorted(DOC_PATH.glob(\"*.txt\")):\n",
    "    law_text = law_file.read_text(encoding=\"utf-8\")\n",
    "    articles = split_articles(law_text)  # drop preamble\n",
    "\n",
    "    example_idx = random.randrange(len(articles))\n",
    "    article_key = list(articles.keys())[example_idx]\n",
    "    print(f\"\\n--- {law_file.name}, total articles {len(articles)} ---\")\n",
    "    print(article_key)\n",
    "    print(wrap_at_spaces(articles[article_key],width=100))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ddb1b6e",
   "metadata": {},
   "source": [
    "### Count tokens per file.\n",
    "\n",
    "Given our working dataset, we show the number of tokens per law text below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1dbf9263",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Algemene ouderdomswet.txt: 25883 tokens\n",
      "Pensioenwet.txt: 126337 tokens\n",
      "Uitvoeringsbesluit loonbelasting 1965.txt: 20766 tokens\n",
      "Wet op loonbelasting 1964.txt: 64790 tokens\n",
      "\n",
      "Total tokens across all files: 237776\n"
     ]
    }
   ],
   "source": [
    "# Token count\n",
    "from utils import count_tokens_in_docs\n",
    "\n",
    "print(count_tokens_in_docs())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4280cb3d",
   "metadata": {},
   "source": [
    "## 2 – Direct LLM retrieval vs. Article‑wise retrieval  \n",
    "We compare:\n",
    "\n",
    "1. **Whole‑law prompt** – push the complete text (~60 k tokens)                                                 → costly ❌ fast ✔️ accurate ❌  \n",
    "2. **Chunked/article prompts** – iterate per article                                                            → costly ❌ slow ❌ accurate ✔️  \n",
    "3. **RAG** - use article text embeddings to identify five most relevant articles before searching per article   → cheap ✔️ fast ✔️ accurate ✔️  \n",
    "4. **Graph RAG** - Using a law text's linked-data structure                                                     → cheap ✔️ fast ✔️ accurate ✔️\n",
    "\n",
    "We demonstrates examples 3 and 4 at the end of the notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27e3fb2b",
   "metadata": {},
   "source": [
    "### 2.1. First, using 'naive' full text chunking in one large system prompt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d8fb1b0f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q Wat is de franchise, welk artikel gebruik je ervoor en wat zijn uitzonderingen op de wettelijke waarde? Hou het antwoord onder de 50 woorden.\n",
      "A: De franchise bedraagt € 18.475 volgens artikel 18a, tweede lid. Uitzonderingen op de wettelijke\n",
      "waarde zijn niet van toepassing als de regeling zoals bedoeld in artikel 18 niet voldoet aan de\n",
      "normeringen en begrenzingen in hoofdstuk IIB.\n",
      "\n",
      "--- Performance ---\n",
      "{\n",
      "  \"scenario\": \"Hele wetstekst ingeladen\",\n",
      "  \"input tokens\": 48515,\n",
      "  \"output tokens\": 55,\n",
      "  \"USD cost\": 0.01462,\n",
      "  \"elapsed seconds\": 2.4\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import json\n",
    "from utils import gpt_4o_mini, llm_metrics,wrap_at_spaces\n",
    "\n",
    "QUESTION = \"Wat is de franchise, welk artikel gebruik je ervoor en wat zijn uitzonderingen op de wettelijke waarde? Hou het antwoord onder de 50 woorden.\"\n",
    "\n",
    "# --- Whole law ---\n",
    "with open(\"data/texts/Wet op loonbelasting 1964.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "    law_text = f.read()\n",
    "    start = time.time()\n",
    "    response = gpt_4o_mini(user_message=QUESTION,law_text = law_text)\n",
    "\n",
    "    elapsed = time.time()-start\n",
    "    out_tokens = response.usage.completion_tokens\n",
    "    in_tokens = response.usage.prompt_tokens\n",
    "    print(f\"Q {QUESTION}\")\n",
    "    print(f\"A: {wrap_at_spaces(response.choices[0].message.content,100)}\")\n",
    "    print(f\"\\n--- Performance ---\")\n",
    "    print(json.dumps(llm_metrics(\"Hele wetstekst ingeladen\", in_tokens, out_tokens, elapsed),indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65893cad",
   "metadata": {},
   "source": [
    "#### Answer incorrect!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5764d5f1",
   "metadata": {},
   "source": [
    "### 2.2. Second, by looping per article section."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cbe405be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q Wat is de franchise, en wat zijn uitzonderingen op de wettelijke waarde? Indien deze tekst hier geen expliciete informatie over geeft, antwoord met enkel 'None'. Hou het antwoord onder de 50 woorden.\n",
      "A: De franchise bedraagt € 18.475 en kan jaarlijks bij ministeriële regeling worden aangepast.\n",
      "Uitzonderingen zijn mogelijk indien een lager percentage per dienstjaar wordt toegepast, waardoor\n",
      "een lager bedrag in aanmerking kan worden genomen.\n",
      "\n",
      "--- Performance ---\n",
      "{\n",
      "  \"scenario\": \"Hele wetstekst ingeladen\",\n",
      "  \"input tokens\": 59330,\n",
      "  \"output tokens\": 440,\n",
      "  \"USD cost\": 0.018327,\n",
      "  \"elapsed seconds\": 39.608\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import json\n",
    "from utils import gpt_4o_mini,llm_metrics,wrap_at_spaces,split_articles\n",
    "\n",
    "QUESTION = \"Wat is de franchise, en wat zijn uitzonderingen op de wettelijke waarde? \"+\\\n",
    "\"Indien deze tekst hier geen expliciete informatie over geeft, antwoord met enkel 'None'. Hou het antwoord onder de 50 woorden.\"\n",
    "\n",
    "start = time.time()\n",
    "in_tokens,out_tokens = 0,0\n",
    "answers = []\n",
    "with open(\"data/texts/Wet op loonbelasting 1964.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "    law_text = f.read()\n",
    "    article_dict = split_articles(law_text)\n",
    "    for key, law_article_text in article_dict.items():\n",
    "        response = gpt_4o_mini(user_message=QUESTION,law_text = law_article_text)\n",
    "        in_tokens += response.usage.prompt_tokens\n",
    "        out_tokens += response.usage.completion_tokens\n",
    "        if \"None\" in str(response.choices[0].message.content):\n",
    "            continue\n",
    "        answers.append((key,response.choices[0].message.content))\n",
    "    elapsed = time.time()-start\n",
    "    llm_metrics(\"Zoeken per artikel\", in_tokens, out_tokens, elapsed)\n",
    "    for key,ans in answers:\n",
    "        print(f\"Q {QUESTION}\")\n",
    "        print(f\"A: {wrap_at_spaces(ans,100)}\")\n",
    "        print(f\"\\n--- Performance ---\")\n",
    "        print(json.dumps(llm_metrics(\"Hele wetstekst ingeladen\", in_tokens, out_tokens, elapsed),indent=2))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ff9d673",
   "metadata": {},
   "source": [
    "#### Answer is correct, but takes a long time to find (50+ seconds)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68a2b4fa",
   "metadata": {},
   "source": [
    "\n",
    "**Main observations**\n",
    "\n",
    "- The franchise value can be lower than € 18.475 given the premium percentage, this is only observed when looping over articles.\n",
    "- Looping over articles is accurate but takes a long time (50+ seconds in contrast to approximately 3 seconds). Both methods are costly, token-wise."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef616e29",
   "metadata": {},
   "source": [
    "## 3. Introduction to RAG"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9d49064",
   "metadata": {},
   "source": [
    "\n",
    "A word/sentence embedding is a numerical representation (using a vector) of it's semantic meaning. \n",
    "\n",
    "By creating vector-stores of texts first, one no longer has to rely on word-matching or brute force loop-searching to find the right article containing specific text\n",
    "by simply searching over a small subset of articles with an embedding closest to the question at hand, which is RAG in a nutshell. \n",
    "\n",
    "Cosine similarity measures the (L2) distance between two vectors, in this case word/sentence embeddings. \n",
    "Values are between 0 and 1, with values closer to 1 indicating words/sentences that are similar."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06f35043",
   "metadata": {},
   "source": [
    "### 3.1. Comparing embeddings ofwords/sentences."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d3ec2bb",
   "metadata": {},
   "source": [
    "Change the model parameters to observe differences across similarity metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "55c56a6a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- model: mxbai-embed-large-v1 ---\n",
      "t1: car\n",
      "t2: cat\n",
      "distance 0.7\n",
      "t1: cat \n",
      "t2: kitten \n",
      "distance 0.83\n",
      "t1: cat is a large animal\n",
      "t2: kitten is a large animal\n",
      "distance 0.89\n",
      "t1: cat is a large animal with much fur\n",
      "t2: kitten is a large animal with much fur\n",
      "distance 0.91\n",
      "---\n",
      "t1: rock\n",
      "t2: object that beats scissors in rock paper scissors\n",
      "distance 0.56\n",
      "t1: Tallest building in New York in 1931\n",
      "t2: The Empire State Building\n",
      "distance 0.68\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from utils import text_embedding_3_large\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "\n",
    "# Adjustable parameters\n",
    "model = \"mxbai-embed-large-v1\" #\"mxbai-embed-large-v1\" or \"text_embedding_3_large\"\n",
    "\n",
    "assert model in [\"mxbai-embed-large-v1\",\"text_embedding_3_large\"], \"model must be either 'hf' or 'azure'\"\n",
    "mxbai_emb = HuggingFaceEmbeddings(model_name=\"mixedbread-ai/mxbai-embed-large-v1\")\n",
    "#Test one, same sentence, one different word.\n",
    "print( \"--- model:\",model,\"---\")\n",
    "text_tuple_list = list([(\"car\",\"cat\")]+[(f\"cat {sentence}\",f\"kitten {sentence}\") for sentence in [\"\",\"is a large animal\", \"is a large animal with much fur\"]])\n",
    "for t1,t2 in text_tuple_list:\n",
    "    if model == \"mxbai-embed-large-v1\":\n",
    "        encoder_response = mxbai_emb.embed_documents([t1,t2])\n",
    "        v1,v2 = (np.array(encoder_response[i]).reshape(1,-1) for i in [0,1])\n",
    "    else:\n",
    "        encoder_response = text_embedding_3_large([t1,t2])\n",
    "        v1,v2 = (np.array(encoder_response.data[i].embedding).reshape(1,-1) for i in [0,1])\n",
    "    print(f\"t1: {t1}\")\n",
    "    print(f\"t2: {t2}\")\n",
    "    print(\"distance\",np.round(cosine_similarity(v1,v2)[0,0],2))\n",
    "\n",
    "#Test two, same meaning, different words.\n",
    "print( \"---\")\n",
    "text_tuple_list = list([(\"rock\",\"object that beats scissors in rock paper scissors\"),(\"Tallest building in New York in 1931\",\"The Empire State Building\")])\n",
    "for t1,t2 in text_tuple_list:\n",
    "    if model == \"mxbai-embed-large-v1\":\n",
    "        encoder_response = mxbai_emb.embed_documents([t1,t2])\n",
    "        v1,v2 = (np.array(encoder_response[i]).reshape(1,-1) for i in [0,1])\n",
    "    else:\n",
    "        encoder_response = text_embedding_3_large([t1,t2])\n",
    "        v1,v2 = (np.array(encoder_response.data[i].embedding).reshape(1,-1) for i in [0,1])\n",
    "    print(f\"t1: {t1}\")\n",
    "    print(f\"t2: {t2}\")\n",
    "    print(\"distance\",np.round(cosine_similarity(v1,v2)[0,0],2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "caccdc5c",
   "metadata": {},
   "source": [
    "**This cell demonstrates two things:**\n",
    "\n",
    "When keeping one word different but increasing the sentence size, the differing word leads to a smaller difference between the resulting embeddings.\n",
    "\n",
    "Distance between 'similar' sentences using the 'text_embedding_3_large' model seem to be further apart compared to the HuggingFaceEmbeddings 'mxbai-embed-large-v1' model.\n",
    "\n",
    "This can be tested by adjusting the 'model' parameter above."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32857fec",
   "metadata": {},
   "source": [
    "### 3.2. Creating law article vector stores using both models (takes 3+ minutes on my developer-laptop), do not rerun."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ad8b86d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from pathlib import Path\n",
    "# from langchain_community.vectorstores import FAISS\n",
    "# from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "# from utils import AZURE_EMBEDDINGS, MXBAI_EMBEDDINGS, split_articles\n",
    "# import os\n",
    "\n",
    "# dir_name = os.getcwd()\n",
    "\n",
    "# # Save law texts seperately to a list and create the underlying vector_stores\n",
    "# DOC_PATH = Path(\"data/texts\")\n",
    "# article_list = []\n",
    "# for law_file in sorted(DOC_PATH.glob(\"*.txt\")):\n",
    "#     law_text = law_file.read_text(encoding=\"utf-8\")\n",
    "#     articles = split_articles(law_text)  # drop preamble\n",
    "#     article_list += [f\"\"\"{law_file.name} {key} {value}\"\"\" for key,value in articles.items()]\n",
    "\n",
    "# # 20000 chosen because the largest law text is 18000 characters.\n",
    "# splitter = RecursiveCharacterTextSplitter(chunk_size=20000, chunk_overlap=0)\n",
    "# documents = splitter.create_documents(article_list)\n",
    "\n",
    "# # 'Vector stores', for now not stored efficiently (using FAISS langchain).\n",
    "# mxbai_store = FAISS.from_documents(documents, MXBAI_EMBEDDINGS,normalize_L2=True)\n",
    "# mxbai_store.save_local(os.path.join(dir_name,\"data/vector_stores/mxbai-embed-large-v1_nongraph.index\"))\n",
    "# azure_store = FAISS.from_documents(documents, AZURE_EMBEDDINGS,normalize_L2=True)\n",
    "# azure_store.save_local(os.path.join(dir_name,\"data/vector_stores/text-embedding-3-large_nongraph.index\"))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "028e7bc3",
   "metadata": {},
   "source": [
    "## 4. Demonstrating RAG using article vector store."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f87da14",
   "metadata": {},
   "source": [
    "### 4.1. For the generated vector store shows 5 of the most closely matched articles to the original question."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23e8d9b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Top 5 --- mixedbread-ai/mxbai-embed-large-v1\n",
      "1. Wet op loonbelasting 1964.txt Artikel 38s 1. Indien een pensioenregeling op grond van artikel 150f van de Pensioenwet of artikel 145e van de Wet [...]\n",
      "2. Wet op loonbelasting 1964.txt Artikel 18a 1. De premie per dienstjaar voor een ouderdomspensioen en een partnerpensioen bij overlijden op of na [...]\n",
      "3. Pensioenwet.txt Artikel 130. Vermelding premie in jaarrekening en bestuursverslag [Vervallen per 01-07-2023]\n",
      "4. Wet op loonbelasting 1964.txt Artikel 38m [Vervallen per 01-04-2017]\n",
      "5. Wet op loonbelasting 1964.txt Artikel 38j [Vervallen per 01-04-2017]\n",
      "\n",
      "--- Top 5 --- text_embedding_3_large\n",
      "1. Wet op loonbelasting 1964.txt Artikel 18a 1. De premie per dienstjaar voor een ouderdomspensioen en een partnerpensioen bij overlijden op of na [...]\n",
      "2. Pensioenwet.txt Artikel 150l. Standaard invaarpad 1. De wijze waarop wordt omgegaan met opgebouwde pensioenaanspraken en pensioenrechten als [...]\n",
      "3. Wet op loonbelasting 1964.txt Artikel 13 1. Niet in geld genoten loon wordt in aanmerking genomen naar de waarde die daaraan in het economische [...]\n",
      "4. Pensioenwet.txt Artikel 67. Afkoop klein partnerpensioen of wezenpensioen bij ingang 1. De pensioenuitvoerder heeft jegens de nabestaanden het [...]\n",
      "5. Pensioenwet.txt Artikel 79. Plicht tot waardeaanwending bij keuzerecht of keuzemogelijkheid 1. De pensioenuitvoerder is verplicht om op verzoek [...]\n"
     ]
    }
   ],
   "source": [
    "import textwrap\n",
    "from utils import MXBAI_STORE_NONGRAPH, AZURE_STORE_NONGRAPH, topk\n",
    "\n",
    "# number of articles to check\n",
    "n = 5\n",
    "\n",
    "QUESTION = \"Wat is de franchise, welk artikel gebruik je ervoor en wat zijn uitzonderingen op de wettelijke waarde? Hou het antwoord onder de 50 woorden.\"\n",
    "v1 = topk(MXBAI_STORE_NONGRAPH,QUESTION,k=n)\n",
    "v2 = topk(AZURE_STORE_NONGRAPH,QUESTION,k=n)\n",
    "\n",
    "article_list_mxbai = [tup[0].page_content for tup in v1]\n",
    "article_list_azure = [tup[0].page_content for tup in v2]\n",
    "\n",
    "# Pair them distance-first so heapq.nlargest uses the distance as the key\n",
    "dist_list = list(zip([\"mixedbread-ai/mxbai-embed-large-v1\",\"text_embedding_3_large\"],[article_list_mxbai, article_list_azure]))\n",
    "for name,article_list in dist_list:\n",
    "    print(f\"\\n--- Top {n} --- {name}\")\n",
    "    for i, article in enumerate(article_list):\n",
    "        print(f\"{i+1}.\",f\"{textwrap.shorten(article,150)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17d60006",
   "metadata": {},
   "source": [
    "#### Article 18a of 'Wet op loonbelasting 1964' is the correct article.\n",
    "The 'text_embedding_3_large' encoder ranks it first, 'mixedbread-ai/mxbai-embed-large-v1' model ranks it second."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd47ac57",
   "metadata": {},
   "source": [
    "### 4.2. Putting everything together, using mxbai encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "339e4ea9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q: Wat is de franchise, welk artikel gebruik je ervoor en wat zijn uitzonderingen op de wettelijke waarde? Hou het antwoord onder de 50 woorden.\n",
      "A: De franchise bedraagt € 18.475, zoals vermeld in Artikel 18a, lid 3. Uitzonderingen zijn mogelijk bij deeltijdwerk, en een lager bedrag kan worden toegepast als een lager percentage per dienstjaar wordt gebruikt.\n",
      "\n",
      "--- Top 5 documents ---\n",
      "1. Wet op loonbelasting 1964.txt Artikel 38s 1. Indien een pensioenregeling op grond van artikel 150f van de Pensioenwet of artikel 145e van de Wet [...]\n",
      "2. Wet op loonbelasting 1964.txt Artikel 18a 1. De premie per dienstjaar voor een ouderdomspensioen en een partnerpensioen bij overlijden op of na [...]\n",
      "3. Pensioenwet.txt Artikel 130. Vermelding premie in jaarrekening en bestuursverslag [Vervallen per 01-07-2023]\n",
      "4. Wet op loonbelasting 1964.txt Artikel 38m [Vervallen per 01-04-2017]\n",
      "5. Wet op loonbelasting 1964.txt Artikel 38j [Vervallen per 01-04-2017]\n",
      "\n",
      "--- Performance ---\n",
      "{\n",
      "  \"scenario\": \"RAG op wetsartikelen\",\n",
      "  \"input tokens\": 1708,\n",
      "  \"output tokens\": 50,\n",
      "  \"USD cost\": 0.000572,\n",
      "  \"elapsed seconds\": 2.376\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import textwrap\n",
    "import json\n",
    "from utils import rag_executor, MXBAI_STORE_NONGRAPH\n",
    "\n",
    "start = time.time()\n",
    "QUESTION = \"Wat is de franchise, welk artikel gebruik je ervoor en wat zijn uitzonderingen op de wettelijke waarde? Hou het antwoord onder de 50 woorden.\"\n",
    "answer,top_docs,qa_chain,performance_cache = rag_executor(QUESTION,store = MXBAI_STORE_NONGRAPH)\n",
    "print(\"Q:\", QUESTION)\n",
    "print(\"A:\", answer)\n",
    "print(f\"\\n--- Top {len(top_docs)} documents ---\")\n",
    "for i,doc in enumerate(top_docs):\n",
    "    print(f\"{i+1}.\",f\"{textwrap.shorten(str(doc.page_content),150)}\")\n",
    "\n",
    "print(f\"\\n--- Performance ---\")\n",
    "in_tokens = performance_cache[\"in_tokens\"]\n",
    "out_tokens = performance_cache[\"out_tokens\"]\n",
    "elapsed = performance_cache[\"elapsed\"]\n",
    "print(json.dumps(llm_metrics(\"RAG op wetsartikelen\", in_tokens, out_tokens, elapsed),indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5c70f58",
   "metadata": {},
   "source": [
    "#### Much faster and accurate compared to the methods in Section 2.1 or 2.2."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac8d4a49",
   "metadata": {},
   "source": [
    "## 5. Graph RAG, taking advantage of linked data structure for RAG."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8831d282",
   "metadata": {},
   "source": [
    "### 5.1. Law graph (created externally)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c688dc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO: Placeholder, visualize graph.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ca18065",
   "metadata": {},
   "source": [
    "### 5.2. Graph-based vector store."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2605c15",
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO: Placeholder, create vector stores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "691a423d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO: Placeholder, display top 5 articles based on a query."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abf3321d",
   "metadata": {},
   "source": [
    "### 5.3. Questions across articles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b372a125",
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO: Graph RAG demonstration."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb3808b3",
   "metadata": {},
   "source": [
    "## 6. Wrap‑up / Key takeaways ✅  \n",
    "\n",
    "* **Direct prompting** on entire laws is cost‑heavy and hits context limits.  \n",
    "* **Chunking** improves alignment but sacrifices latency.  \n",
    "* **RAG** with a high‑quality embedding model (MXBAI) gives the *best accuracy‑per‑dollar*.  \n",
    "* Integrating domain‑specific knowledge graphs can further boost recall for cross‑article references.  \n",
    "\n",
    "Feel free to extend the notebook by  \n",
    "* replacing placeholder accuracies with manual grading,  \n",
    "* adding caching for embeddings,  \n",
    "* deploying the FAISS index as an API.  \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
