{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "69e56202",
   "metadata": {},
   "source": [
    "# RAG Workshop â€“ Dutch Pension Law  \n",
    "Author: *Generated on 2025-04-30*  \n",
    "\n",
    "This notebook accompanies an internal workshop on **Retrievalâ€‘Augmented Generation (RAG)** with Dutch pensionâ€‘related law texts from [wetten.overheid.nl](https://wetten.overheid.nl).  \n",
    "You will need:\n",
    "\n",
    "* PythonÂ 3.10+  \n",
    "* `openai`, `azureâ€‘identity`, `langchain`, `langchainâ€‘community`, `langchainâ€‘huggingface`, `tiktoken`, `networkx`, `rdflib`  \n",
    "\n",
    "From an empty source folder in which you will run the notebook, run the following commands:\n",
    "\n",
    "1. **Clone & install**\n",
    "   ```bash\n",
    "   git init\n",
    "   python -m venv .venv && source .venv/bin/activate\n",
    "   git clone https://github.com/iAMLAB-test/workshop_llm\n",
    "   pip install -r requirements.txt\n",
    "   ```\n",
    "\n",
    "Using `config_template.yaml` fill it's contexts (provided seperately) and save under `config.yaml`:\n",
    "\n",
    "2. **Configure environment**\n",
    "   ```bash\n",
    "   API_KEY: [Enter your API key here]\n",
    "   AZURE_ENDPOINT: [Enter your Azure endpoint here]\n",
    "   API_VERSION: [Enter your API version here]\n",
    "   ```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfd88990",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d3c1e4d",
   "metadata": {},
   "source": [
    "### Simple LLM call."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "e1039e6a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sure! Hereâ€™s \"Hello world\" in five different languages:\n",
      "\n",
      "1. Spanish: Â¡Hola mundo!\n",
      "2. French: Bonjour le monde !\n",
      "3. German: Hallo Welt!\n",
      "4. Italian: Ciao mondo!\n",
      "5. Japanese: ã“ã‚“ã«ã¡ã¯ä¸–ç•Œ (Konnichiwa sekai) \n",
      "\n",
      "Let me know if you need more!\n"
     ]
    }
   ],
   "source": [
    "#Prompting API example.\n",
    "import importlib,utils\n",
    "importlib.reload(utils)\n",
    "from utils import gpt_4o_mini\n",
    "\n",
    "query = \"Say 'Hello world' in five different languages.\"\n",
    "print(gpt_4o_mini(query).choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81016e16",
   "metadata": {},
   "source": [
    "## 1Â â€“Â Layout of Dutch law texts  \n",
    "We will inspect local `.txt` dumps in `data/docs/`. Each file contains **one complete law** (some >Â 60â€¯k tokens).\n",
    "\n",
    "Below we load the all three files and print a *single article* per file to understand the structure."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f16bfcf",
   "metadata": {},
   "source": [
    "### Random article from input law texts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "7d70b85b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Pensioenswet.txt, total articles 323 ---\n",
      "Artikel 28.\n",
      "Melding door pensioenfonds inzake premieachterstand  1.      Een pensioenfonds informeert elk\n",
      "kwartaal schriftelijk het verantwoordingsorgaan of het belanghebbendenorgaan en, bij het ontbreken\n",
      "daarvan, de deelnemers, gewezen deelnemers en pensioengerechtigden wanneer sprake is van een\n",
      "premieachterstand ter grootte van 5% van de totale door het pensioenfonds te ontvangen jaarpremie.\n",
      "2.      Gedurende de in het eerste lid bedoelde situatie informeert een pensioenfonds tevens elk\n",
      "kwartaal de ondernemingsraad van de onderneming die nog premie aan het pensioenfonds verschuldigd\n",
      "is.  3.      Bij een algemeen pensioenfonds worden de voorgaande twee leden toegepast per\n",
      "afgescheiden vermogen.\n",
      "\n",
      "--- Uitvoeringsbesluit loonbelasting 1965.txt, total articles 50 ---\n",
      "Artikel 10b\n",
      "1.      Als loonbestanddelen als bedoeld in artikel 18g, tweede lid, onderdeel a, van de wet komen\n",
      "in aanmerking:         a.      alle loonbestanddelen, met uitzondering van het genot van een ter\n",
      "beschikking gestelde auto;         b.      ingehouden bedragen als bedoeld in artikel 11, eerste\n",
      "lid, onderdeel j, van de wet;         c.      loonbestanddelen die worden geruild tegen een\n",
      "vermindering van de arbeidstijd tot een maximum van 10% van de overeengekomen arbeidsduur, mits:\n",
      "1Â°.     De mogelijkheid van deze ruil schriftelijk is vastgelegd in een regeling waaraan ten minste\n",
      "driekwart van de werknemers kan deelnemen die behoren tot een organisatorische of functionele\n",
      "eenheid van de inhoudingsplichtige;                 2Â°.     Het een regeling betreft waarbij de\n",
      "verlaging van het loon tijdelijk is; en                 3Â°.     De werknemer ten minste Ã©Ã©n keer per\n",
      "jaar de keuze heeft om de samenstelling van zijn loon te wijzigen.  2.      In afwijking van het\n",
      "eerste lid, onderdeel a, komen eindheffingsbestanddelen als bedoeld in artikel 31 van de wet slechts\n",
      "in aanmerking, indien deze bestanddelen geÃ¯ndividualiseerd zijn.  3.      In afwijking van het\n",
      "eerste lid, onderdeel a, mogen voor de toepassing van de artikelen 18b en 18c van de wet regelmatig\n",
      "genoten loonbestanddelen worden gesteld op het gemiddelde van die regelmatig genoten\n",
      "loonbestanddelen in ten hoogste de laatste vijf kalenderjaren direct voorafgaande aan het\n",
      "kalenderjaar van overlijden van de werknemer. De eerste zin is van overeenkomstige toepassing op\n",
      "niet regelmatig genoten loonbestanddelen.  4.      Gedurende perioden van onbetaald verlof als\n",
      "bedoeld in artikel 10a, tweede lid, wordt voor de toepassing van het eerste lid uitgegaan van de\n",
      "direct voorafgaande aan die perioden genoten loonbestanddelen, dan wel de direct na afloop van die\n",
      "perioden genoten loonbestanddelen.  5.      Gedurende perioden na onvrijwillig ontslag waarin\n",
      "loongerelateerde uitkeringen worden ontvangen als bedoeld in artikel 10a, eerste lid, onderdeel c,\n",
      "wordt voor de toepassing van het eerste lid uitgegaan van de direct voorafgaande aan die perioden\n",
      "genoten loonbestanddelen uit tegenwoordige dienstbetrekking.  6.      Voor de toepassing van het\n",
      "eerste lid, onderdeel a, wordt gedurende andere perioden na ontslag dan bedoeld in het vijfde lid,\n",
      "uitgegaan van de direct voorafgaande aan die perioden genoten loonbestanddelen uit tegenwoordige\n",
      "dienstbetrekking. De eerste zin geldt voor een periode van ten hoogste drie jaar. Vanaf het vierde\n",
      "kalenderjaar na ontslag wordt geen hoger bedrag als pensioengevend loon in aanmerking genomen dan\n",
      "het door de gewezen werknemer in het tweede kalenderjaar voorafgaand aan het kalenderjaar genoten\n",
      "gezamenlijke bedrag van:         1Â°.     De winst uit onderneming vÃ³Ã³r de ondernemersaftrek;\n",
      "2Â°.     De genoten loonbestanddelen uit tegenwoordige dienstbetrekking;         3Â°.     Het\n",
      "belastbare resultaat uit overige werkzaamheden; en         4Â°.     De belastbare periodieke\n",
      "uitkeringen en verstrekkingen.  7.      Voor de toepassing van artikel 18g, tweede lid, onderdeel b,\n",
      "van de wet mag een loonsverlaging buiten beschouwing blijven, voor zover deze het gevolg is van het\n",
      "terugtreden naar een lager gekwalificeerde functie, in de periode die aanvangt 10 jaar direct\n",
      "voorafgaande aan de in de pensioenregeling vastgestelde ingangsdatum.  8.      Voor de toepassing\n",
      "van artikel 18g, tweede lid, onderdeel c, van de wet mag een loonsverlaging buiten beschouwing\n",
      "blijven, voor zover deze het gevolg is van ziekte of arbeidsongeschiktheid.  9.      Voor de\n",
      "toepassing van het vierde tot en met achtste lid kan het loon gedurende de aldaar bedoelde perioden\n",
      "worden geÃ¯ndexeerd met de loonindex in de bedrijfstak waarin de werknemer werkzaam is, dan wel met\n",
      "de gemiddelde loonindex voor de CAO-lonen per maand, inclusief bijzondere beloningen, zoals berekend\n",
      "door het Centraal Bureau voor de Statistiek.\n",
      "\n",
      "--- Wet op loonbelasting 1964.txt, total articles 169 ---\n",
      "Artikel 28c\n",
      "1.      Indien de inhoudingsplichtige de opgave, bedoeld in artikelÂ 28, eerste lid, onderdeelÂ g,\n",
      "niet, onjuist, onvolledig dan wel niet binnen de gestelde termijn heeft verstrekt, vormt dit een\n",
      "verzuim terzake waarvan de inspecteur hem een bestuurlijke boete van ten hoogste â‚¬Â 1.675 kan\n",
      "opleggen.  2.      De bevoegdheid tot het opleggen van een bestuurlijke boete wegens het feit,\n",
      "bedoeld in het eerste lid, vervalt door verloop van Ã©Ã©n jaar na het einde van het kalenderjaar\n",
      "waarin de opgave, bedoeld in artikelÂ 28, eerste lid, onderdeelÂ g, had moeten worden verstrekt.  3.\n",
      "Artikel 67cb van de Algemene wet inzake rijksbelastingen is van overeenkomstige toepassing op het in\n",
      "het eerste lid genoemde bedrag.\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "import random\n",
    "import importlib,utils\n",
    "importlib.reload(utils)\n",
    "from utils import *\n",
    "\n",
    "# Load a law text file.\n",
    "DOC_PATH = Path(\"data/docs\")\n",
    "for law_file in sorted(DOC_PATH.glob(\"*.txt\")):\n",
    "    law_text = law_file.read_text(encoding=\"utf-8\")\n",
    "    articles = split_articles(law_text)  # drop preamble\n",
    "\n",
    "    example_idx = random.randrange(len(articles))\n",
    "    article_key = list(articles.keys())[example_idx]\n",
    "    print(f\"\\n--- {law_file.name}, total articles {len(articles)} ---\")\n",
    "    print(article_key)\n",
    "    print(wrap_at_spaces(articles[article_key],width=100))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ddb1b6e",
   "metadata": {},
   "source": [
    "### Count tokens per file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "1dbf9263",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pensioenswet.txt: 126338 tokens\n",
      "Uitvoeringsbesluit loonbelasting 1965.txt: 20766 tokens\n",
      "Wet op loonbelasting 1964.txt: 64790 tokens\n",
      "\n",
      "Total tokens across all files: 211894\n"
     ]
    }
   ],
   "source": [
    "# Token count\n",
    "import importlib, utils\n",
    "importlib.reload(utils)\n",
    "from utils import count_tokens_in_docs\n",
    "\n",
    "print(count_tokens_in_docs())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4280cb3d",
   "metadata": {},
   "source": [
    "## 2Â â€“Â Direct LLM retrieval vs. Articleâ€‘wise retrieval  \n",
    "We compare:\n",
    "\n",
    "1. **Wholeâ€‘law prompt** â€“ push the complete text (~60â€¯k tokens) â†’ costly âœ”ï¸ fast âŒ accurate âŒ  \n",
    "2. **Chunked/article prompts** â€“ iterate per article â†’ costly âŒ slow âŒ accurate âœ”ï¸  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8fb1b0f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"scenario\": \"whole_law_single_call\",\n",
      "  \"input tokens\": 48514,\n",
      "  \"output tokens\": 62,\n",
      "  \"USD cost\": 0.014629,\n",
      "  \"elapsed seconds\": 3.135,\n",
      "  \"accuracy\": null\n",
      "}\n",
      "De franchise bedraagt â‚¬ 18.475 en wordt vastgesteld in artikel 18a, tweede lid. Uitzonderingen op de\n",
      "wettelijke waarde zijn opgenomen in artikel 18a, eerste lid, en hebben betrekking op specifieke\n",
      "situaties zoals arbeidsongeschiktheid en premies voor bepaalde pensioenregelingen.\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import importlib,utils\n",
    "importlib.reload(utils)\n",
    "from utils import gpt_4o_mini, SYSTEM,show_metrics,wrap_at_spaces\n",
    "\n",
    "QUESTION = \"Wat is de franchise, welk artikel gebruik je ervoor en wat zijn uitzonderingen op de wettelijke waarde? Hou het antwoord onder de 50 woorden.\"\n",
    "\n",
    "# --- Whole law ---\n",
    "with open(\"data/docs/Wet op loonbelasting 1964.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "    law_text = f.read()\n",
    "    start = time.time()\n",
    "    response = gpt_4o_mini(user_message=QUESTION,system_message=SYSTEM.format(law_text = law_text))\n",
    "\n",
    "    elapsed = time.time()-start\n",
    "    out_tokens = response.usage.completion_tokens\n",
    "    in_tokens = response.usage.prompt_tokens\n",
    "    ans1 = response.choices[0].message.content\n",
    "    show_metrics(\"Hele wetstekst ingeladen\", in_tokens, out_tokens, elapsed)\n",
    "    print(wrap_at_spaces(ans1,100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbe405be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"scenario\": \"whole_law_single_call\",\n",
      "  \"input tokens\": 59161,\n",
      "  \"output tokens\": 454,\n",
      "  \"USD cost\": 0.018293,\n",
      "  \"elapsed seconds\": 51.567\n",
      "}\n",
      "\n",
      "--- Artikel 18a ---\n",
      "De franchise bedraagt â‚¬ 18.475, maar kan jaarlijks worden aangepast. Een lager bedrag mag in\n",
      "aanmerking worden genomen indien een lager percentage per dienstjaar wordt toegepast dan de\n",
      "wettelijke waarde.\n",
      "\n",
      "--- Artikel 31a ---\n",
      "De tekst geeft geen expliciete informatie over de franchise of uitzonderingen op de wettelijke\n",
      "waarde.\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import importlib,utils\n",
    "importlib.reload(utils)\n",
    "from utils import gpt_4o_mini, SYSTEM, show_metrics, wrap_at_spaces\n",
    "\n",
    "QUESTION = \"Wat is de franchise, en wat zijn uitzonderingen op de wettelijke waarde? \"+\\\n",
    "\"Indien deze tekst hier geen expliciete informatie over geeft, antwoord met enkel 'None'. Hou het antwoord onder de 50 woorden.\"\n",
    "\n",
    "start = time.time()\n",
    "in_tokens,out_tokens = 0,0\n",
    "answers = []\n",
    "with open(\"data/docs/Wet op loonbelasting 1964.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "    law_text = f.read()\n",
    "    article_dict = split_articles(law_text)\n",
    "    for key, value in article_dict.items():\n",
    "        response = gpt_4o_mini(user_message=QUESTION,system_message=SYSTEM.format(law_text = value))\n",
    "        in_tokens += response.usage.prompt_tokens\n",
    "        out_tokens += response.usage.completion_tokens\n",
    "        if \"None\" in str(response.choices[0].message.content):\n",
    "            continue\n",
    "        answers.append((key,response.choices[0].message.content))\n",
    "    elapsed = time.time()-start\n",
    "    show_metrics(\"Zoeken per artikel\", in_tokens, out_tokens, elapsed)\n",
    "    for ans1 in answers:\n",
    "        print(f\"\\n--- {ans1[0]} ---\")\n",
    "        print(wrap_at_spaces(ans1[1],100))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68a2b4fa",
   "metadata": {},
   "source": [
    "**Takeaway:** Looping over articles gives more accurate answers, but takes a long time. \n",
    "Both methods are costly, token-wise."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bec9912e",
   "metadata": {},
   "source": [
    "# Functionâ€‘Calling Experiments\n",
    "\n",
    "The next cell registers a simple calculator tool:\n",
    "Example of many digit computation. A transformer does not 'reason', it infers the answer likely to be the case form a test-corpus."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20aae52a",
   "metadata": {},
   "source": [
    "## 1. Asking ChatGPT directly.\n",
    "Answers are different because a transformer does not 'calculate'.\n",
    "\n",
    "The larger the numbers, the more inaccurate the answers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "39d69a73",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4671300\n",
      "4676130\n"
     ]
    }
   ],
   "source": [
    "import importlib, utils\n",
    "importlib.reload(utils)\n",
    "from utils import gpt_4o_mini\n",
    "\n",
    "x,y = 1255, 3726\n",
    "\n",
    "print(gpt_4o_mini(f\"What is {x} x {y}?, return me only the answer, no seperators\").choices[0].message.content)\n",
    "print(x*y)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b8d38c3",
   "metadata": {},
   "source": [
    "## 2. Using a tool call, infers arguments from the prompt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "1d05859c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4676130\n"
     ]
    }
   ],
   "source": [
    "import importlib, utils\n",
    "importlib.reload(utils)\n",
    "from utils import gpt_4o_mini, tool_schema, multiplier\n",
    "import json\n",
    "\n",
    "x,y = 1255, 3726\n",
    "\n",
    "#Call arguments.\n",
    "arguments = json.loads(gpt_4o_mini(f\"What is {x} x {y}?\",tool_schema=tool_schema).choices[0].message.tool_calls[0].function.arguments)\n",
    "\n",
    "#Calculate answer using the arguments.\n",
    "print(multiplier(arguments[\"x\"],arguments[\"y\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef616e29",
   "metadata": {},
   "source": [
    "# Introduction to RAG  \n",
    "Large calls are brittle â€“ we instead *retrieve* only the most relevant context with **embeddings** and feed that to the LLM."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d83c07b",
   "metadata": {},
   "source": [
    "## 1Â â€“Â Creating word/sentence embeddings & measuring semantic distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e25c8b81",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer, util\n",
    "for model in [\"all-MiniLM-L6-v2\"]:\n",
    "    print(model)\n",
    "\n",
    "    #Baseline similarity\n",
    "    sim = util.cos_sim(SentenceTransformer(model).encode(\"cat\"), SentenceTransformer(model).encode(\"kitten\"))\n",
    "    print(sim[0][0])\n",
    "\n",
    "    #Decreasing attention for word that differs\n",
    "    sim = util.cos_sim(SentenceTransformer(model).encode(\"cat falls from the sky\"), SentenceTransformer(model).encode(\"kitten falls from the sky\"))\n",
    "    print(sim[0][0])\n",
    "\n",
    "    #Decreasing attention for word that differs\n",
    "    sim = util.cos_sim(SentenceTransformer(model).encode(\"cat falls from the sky and lands hard on the ground\"), SentenceTransformer(model).encode(\"kitten falls from the sky and lands hard on the ground\"))\n",
    "    print(sim[0][0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "325f5b11",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import GPT2Tokenizer, GPT2Model\n",
    "from sklearn.metrics.pairwise import cosine_distances\n",
    "\n",
    "tok_gpt2 = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
    "mdl_gpt2 = GPT2Model.from_pretrained(\"gpt2\")\n",
    "texts = [\"De werknemer heeft recht op ouderdomspensioen.\",\n",
    "         \"De werknemer ontvangt een bonus in december.\"]\n",
    "def embed_gpt2(text):\n",
    "    with torch.no_grad():\n",
    "        ids = torch.tensor([tok_gpt2.encode(text)])\n",
    "        return mdl_gpt2(ids).last_hidden_state.mean(dim=1).numpy()\n",
    "\n",
    "embs = [embed_gpt2(t) for t in texts]\n",
    "dist = cosine_distances(embs)[0,1]\n",
    "print(\"GPTâ€‘2 distance:\", dist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6622c97b",
   "metadata": {},
   "outputs": [],
   "source": [
    "show_metrics(\"gpt2_encoder\", sum(map(num_tokens,texts)), 0, 0, accuracy=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55c56a6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def embed_openai(text, model=\"text-embedding-ada-002\"):\n",
    "    resp = openai.Embedding.create(model=model, input=text)\n",
    "    return resp.data[0].embedding\n",
    "\n",
    "embs_ada = [embed_openai(t) for t in texts]\n",
    "dist_ada = cosine_distances(embs_ada)[0,1]\n",
    "print(\"adaâ€‘002 distance:\", dist_ada)\n",
    "show_metrics(\"ada002_embeddings\", sum(map(num_tokens,texts)), 0, 0, accuracy=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53f293d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "hf_emb = HuggingFaceEmbeddings(model_name=\"mixedbread-ai/mxbai-embed-large-v1\")\n",
    "embs_mx = hf_emb.embed_documents(texts)\n",
    "dist_mx = cosine_distances([embs_mx[0]],[embs_mx[1]])[0,0]\n",
    "print(\"MXBAI distance:\", dist_mx)\n",
    "show_metrics(\"mxbai_embeddings\", sum(map(num_tokens,texts)), 0, 0, accuracy=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80e9bd07",
   "metadata": {},
   "source": [
    "## 2Â â€“Â Create a perâ€‘article vector store with MXBAI embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f86986e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.docstore.document import Document\n",
    "from langchain.vectorstores import FAISS\n",
    "\n",
    "docs = [Document(page_content=art, metadata={\"law\":law_file.name, \"article_idx\":i})\n",
    "        for i,art in enumerate(articles)]\n",
    "vectorstore = FAISS.from_documents(docs, hf_emb)\n",
    "print(vectorstore)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff393dd1",
   "metadata": {},
   "source": [
    "## 3Â â€“Â RAG query helper with Azure GPT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a412cb45",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chat_models import AzureChatOpenAI\n",
    "from langchain.chains import ConversationalRetrievalChain\n",
    "\n",
    "llm = AzureChatOpenAI(\n",
    "    openai_api_base=openai.api_base,\n",
    "    deployment_name=DEPLOYMENT,\n",
    "    openai_api_version=openai.api_version,\n",
    "    openai_api_key=openai.api_key,\n",
    "    temperature=0\n",
    ")\n",
    "qa_chain = ConversationalRetrievalChain.from_llm(llm, vectorstore.as_retriever(search_kwargs={\"k\":5}))\n",
    "\n",
    "query = \"Welke artikel legt expliciet de relatie uit tussen de franchise en de pensioenpremie?\"\n",
    "start = time.time()\n",
    "result = qa_chain({\"question\": query, \"chat_history\": []})\n",
    "elapsed = time.time()-start\n",
    "print(result[\"answer\"])\n",
    "# naive tokens\n",
    "in_tokens = num_tokens(query)+sum(num_tokens(d.page_content) for d in result[\"source_documents\"])\n",
    "out_tokens = num_tokens(result[\"answer\"])\n",
    "show_metrics(\"RAG_top5\", in_tokens, out_tokens, elapsed, accuracy=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce95b365",
   "metadata": {},
   "source": [
    "## 4Â â€“Â (Placeholder) Vectorâ€‘store over Knowledge Graph context  \n",
    "The knowledge graph is stored at `data/KG/Law_graph.trig` (RDF TriG).  \n",
    "A typical pipeline looks like:\n",
    "\n",
    "1. Load the graph with **RDFlib**.  \n",
    "2. Derive perâ€‘article contexts by following `skos:related`, `dct:references`, etc.  \n",
    "3. Serialize each article node + referenced texts into a *document* string.  \n",
    "4. Embed with the same MXBAI model and (optionally) push to the same FAISS index or a dedicated one.\n",
    "\n",
    "```python\n",
    "import rdflib, networkx as nx\n",
    "g = rdflib.ConjunctiveGraph().parse(\"data/KG/Law_graph.trig\", format=\"trig\")\n",
    "\n",
    "# build inâ€‘memory NetworkX graph for easier traversal\n",
    "nxg = nx.DiGraph()\n",
    "for s,p,o in g:\n",
    "    if p in (rdflib.SKOS.related, rdflib.RDFS.seeAlso):\n",
    "        nxg.add_edge(str(s), str(o))\n",
    "\n",
    "# TODO: walk nxg from article URI â†’ text file segment\n",
    "```\n",
    "\n",
    "> ðŸ“š See RDFlib docs: <https://rdflib.readthedocs.io/>  \n",
    "> ðŸ“š See LangChain KGâ€‘RAG helpers: `langchain.retrievers.KGTripleRetriever`\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb3808b3",
   "metadata": {},
   "source": [
    "# Wrapâ€‘up / Key takeaways âœ…  \n",
    "\n",
    "* **Direct prompting** on entire laws is costâ€‘heavy and hits context limits.  \n",
    "* **Chunking** improves alignment but sacrifices latency.  \n",
    "* **RAG** with a highâ€‘quality embedding model (MXBAI) gives the *best accuracyâ€‘perâ€‘dollar*.  \n",
    "* Integrating domainâ€‘specific knowledge graphs can further boost recall for crossâ€‘article references.  \n",
    "\n",
    "Feel free to extend the notebook by  \n",
    "* replacing placeholder accuracies with manual grading,  \n",
    "* adding caching for embeddings,  \n",
    "* deploying the FAISS index as an API.  \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
